input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'binary_crossentropy',
optimize = optimizer_rmsprop(),
metrics = c('accuracy'))
nn_binary %>% fit(x = intercept,
y = counts > 0,
epochs = 30,
batch_size = 1024,
validation_split = 0)
glm_binary <- glm((nclaims > 0) ~ 1,
data = mtpl_train,
family = binomial(link = 'logit'))
glm_binary$coefficients
nn_binary$get_weights()[[1]] %>% as.numeric()
unique(predict(glm_binary, type = 'response'))
unique(predict(nn_binary, x = intercept))
glm_offset <- glm(nclaims ~ ageph,
family = poisson(link = 'log'),
data = mtpl_train,
offset = log(expo))
glm_offset$coefficients
glm_weights <- glm(nclaims / expo ~ ageph,
family = poisson(link = 'log'),
data = mtpl_train,
weights = expo)
glm_weights$coefficients
nn_freq_exposure <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'exponential',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
## --------------------------------------------------------------------------------------------------------------------------------------------------
exposure <- mtpl_train$expo
nn_freq_exposure %>%
fit(x = intercept,
y = counts / exposure,
sample_weight = exposure,
epochs = 20,
batch_size = 1024,
validation_split = 0)
ageph <- mtpl_train$ageph
## --------------------------------------------------------------------------------------------------------------------------------------------------
nn_freq_ageph <-
keras_model_sequential() %>%
layer_batch_normalization(input_shape = c(1)) %>%
layer_dense(units = 5,
activation = 'tanh') %>%
layer_dense(units = 1,
activation = 'exponential',
use_bias = TRUE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
## --------------------------------------------------------------------------------------------------------------------------------------------------
nn_freq_ageph %>%
fit(x = ageph,
y = counts / exposure,
sample_weight = exposure,
epochs = 20,
batch_size = 1024,
validation_split = 0)
## --------------------------------------------------------------------------------------------------------------------------------------------------
library(mgcv)
gam_ageph <- gam(nclaims ~ s(ageph),
data = mtpl_train,
family = poisson(link = 'log'),
offset = log(expo))
## --------------------------------------------------------------------------------------------------------------------------------------------------
df_age <- tibble::tibble(
age = 18:95,
NN = as.numeric(predict(nn_freq_ageph, age)),
GAM = predict(gam_ageph, type = 'response',
newdata = data.frame(ageph = age))
)
ggplot(df_age, aes(x = age)) + geom_line(aes(y = GAM)) + geom_point(aes(y = NN)) +
theme_bw() + labs(y = 'Fitted effect') + ggtitle('NN (dots) vs. GAM (line)')
## --------------------------------------------------------------------------------------------------------------------------------------------------
input_skip <- layer_input(shape = c(1), name = 'skip')
input_nn <- layer_input(shape = c(1), name = 'nn')
network <- input_nn %>% layer_batch_normalization() %>% layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 1, activation = 'linear')
output <- list(network, input_skip) %>% layer_add() %>%
layer_dense(units = 1, activation = 'exponential', trainable = FALSE, name = 'output',
weights = list(array(1, dim = c(1,1)), array(0, dim = c(1))))
cann <- keras_model(inputs = list(input_nn, input_skip), outputs = output)
cann %>% compile(loss = 'poisson', optimize = optimizer_rmsprop())
## --------------------------------------------------------------------------------------------------------------------------------------------------
gam_expo <- predict(gam_ageph) + log(mtpl_train$expo)
## --------------------------------------------------------------------------------------------------------------------------------------------------
cann_input <- list('nn' = mtpl_train$ageph,
'skip' = gam_expo)
## --------------------------------------------------------------------------------------------------------------------------------------------------
cann %>% fit(x = cann_input,
y = counts,
epochs = 20,
batch_size = 1024,
validation_split = 0)
## --------------------------------------------------------------------------------------------------------------------------------------------------
df <- tibble::tibble(ageph = 18:95,
skip = 0)
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df) + theme_bw() + geom_point(aes(ageph, effect)) + ggtitle('NN adjustments')
## --------------------------------------------------------------------------------------------------------------------------------------------------
df <- tibble::tibble(ageph = 18:95,
skip = predict(gam_ageph, newdata = as.data.frame(ageph)))
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df, aes(x = ageph)) + geom_point(aes(y = effect)) + geom_line(aes(y = exp(skip))) +
theme_bw() + ggtitle('CANN (dots) vs. GAM (line)')
## --------------------------------------------------------------------------------------------------------------------------------------------------
nn_sev_log <- keras_model_sequential() %>%
layer_dense(units = 1, activation = 'linear',
input_shape = c(1), use_bias = FALSE) %>%
compile(loss = 'mse',
optimize = optimizer_rmsprop())
## --------------------------------------------------------------------------------------------------------------------------------------------------
claims <- mtpl_train %>% dplyr::filter(nclaims > 0)
## --------------------------------------------------------------------------------------------------------------------------------------------------
nn_sev_log %>%
fit(x = rep(1, nrow(claims)),
y = log(claims$avg),
epochs = 100, batch_size = 128,
validation_split = 0)
## --------------------------------------------------------------------------------------------------------------------------------------------------
predict(nn_sev_log, 1) %>% exp() %>% as.numeric()
claims$avg %>% summary()
library(recipes)
# Create and prepare the recipe
mtpl_recipe <- recipe(nclaims ~ ., data = mtpl_train) %>%
step_rm(id, amount, avg, town, pc) %>%
step_nzv(all_predictors(), -expo) %>%
step_normalize(all_numeric(), -c(nclaims, expo)) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
prep(mtpl_train)
# Bake the training and test data
mtpl_train_b <- mtpl_recipe %>% juice()
mtpl_test_b <- mtpl_recipe %>% bake(new_data = mtpl_test)
# Make the data NN proof
train_x <- mtpl_train_b %>%
dplyr::select(-c(nclaims, expo)) %>% as.matrix()
test_x <- mtpl_test_b %>%
dplyr::select(-c(nclaims, expo)) %>% as.matrix()
train_y <- mtpl_train_b$nclaims
test_y <- mtpl_test_b$nclaims
train_expo <- mtpl_train_b$expo
test_expo <- mtpl_test_b$expo
nn_case <- keras_model_sequential() %>%
layer_dense(units = 20,
activation = 'relu',
input_shape = ncol(train_x)) %>%
layer_dense(units = 10,
activation = 'relu') %>%
layer_dense(units = 1,
activation = 'exponential') %>%
compile(loss = 'poisson',
optimize = optimizer_nadam())
nn_case %>%
fit(x = train_x,
y = train_y / train_expo,
sample_weight = train_expo,
epochs = 20,
batch_size = 1024,
validation_split = 0)
nn_case %>%
evaluate(x = test_x,
y = test_y)
# If you want to check the results
poisson_loss <- function(pred, actual) {
mean(pred - actual * log(pred))
}
poisson_loss(predict(nn_case, test_x),
test_y)
# Use as.matrix when using weights in evaluate
nn_case %>%
evaluate(x = test_x,
y = test_y,
sample_weight = array(test_expo))
mtpl <- read.table('../data/PC_data.txt',
header = TRUE, stringsAsFactors = TRUE) %>%
as_tibble() %>% rename_all(tolower) %>% rename(expo = exp)
library(keras)
library(tidyverse)
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
mtpl <- read.table('../data/PC_data.txt',
header = TRUE, stringsAsFactors = TRUE) %>%
as_tibble() %>% rename_all(tolower) %>% rename(expo = exp)
library(rsample)
set.seed(54321)
data_split <- initial_split(mtpl)
mtpl_train <- training(data_split)
mtpl_test  <- testing(data_split)
# Reshuffling of the training observations
mtpl_train <- mtpl_train[sample(nrow(mtpl_train)), ]
nn_freq_intercept <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'exponential',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
nn_freq_intercept <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'exponential',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
nn_freq_intercept$count_params()
intercept <- rep(1, nrow(mtpl_train))
counts <- mtpl_train$nclaims
intercept <- rep(1, nrow(mtpl_train))
counts <- mtpl_train$nclaims
nn_freq_intercept %>% fit(x = intercept,
y = counts,
epochs = 30,
batch_size = 1024,
validation_split = 0,
verbose = 0)
glm_freq_intercept <- glm(nclaims ~ 1,
data = mtpl_train,
family = poisson(link = 'log'))
# GLM coefficients
glm_freq_intercept$coefficients
## NN weights
nn_freq_intercept$weights
## NN weights
nn_freq_intercept$get_weights()
nn_freq_intercept <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'exponential',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
nn_freq_intercept %>% fit(x = intercept,
y = counts,
epochs = 30,
batch_size = 1024,
validation_split = 0,
verbose = 0)
## NN weights
nn_freq_intercept$get_weights()
# GLM coefficients
glm_freq_intercept$coefficients
nn_freq_intercept %>% keras::save_model_tf('nn_freq_intercept')
nn_binary <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'sigmoid',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'binary_crossentropy',
optimize = optimizer_rmsprop(),
metrics = c('accuracy'))
nn_binary %>% fit(x = intercept,
y = counts > 0,
epochs = 30,
batch_size = 1024,
validation_split = 0,
verbose = 0)
glm_binary <- glm((nclaims > 0) ~ 1,
data = mtpl_train,
family = binomial(link = 'logit'))
glm_binary$coefficients
nn_binary$get_weights()[[1]] %>% as.numeric()
unique(predict(glm_binary, type = 'response'))
unique(predict(nn_binary, x = intercept))
nn_binary <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'sigmoid',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'binary_crossentropy',
optimize = optimizer_rmsprop(),
metrics = c('accuracy'))
nn_binary %>% fit(x = intercept,
y = counts > 0,
epochs = 40,
batch_size = 1024,
validation_split = 0,
verbose = 0)
glm_binary$coefficients
nn_binary$get_weights()[[1]] %>% as.numeric()
nn_binary %>% keras::save_model_tf('nn_binary')
glm_offset <- glm(nclaims ~ ageph,
family = poisson(link = 'log'),
data = mtpl_train,
offset = log(expo))
glm_offset$coefficients
glm_weights <- glm(nclaims / expo ~ ageph,
family = poisson(link = 'log'),
data = mtpl_train,
weights = expo)
glm_weights$coefficients
nn_freq_exposure <-
keras_model_sequential() %>%
layer_dense(units = 1,
activation = 'exponential',
input_shape = c(1),
use_bias = FALSE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
exposure <- mtpl_train$expo
nn_freq_exposure %>%
fit(x = intercept,
y = counts / exposure, #<<
sample_weight = exposure, #<<
epochs = 20,
batch_size = 1024,
validation_split = 0,
verbose = 0)
ageph <- mtpl_train$ageph
nn_freq_ageph <-
keras_model_sequential() %>%
layer_batch_normalization(input_shape = c(1)) %>%
layer_dense(units = 5,
activation = 'tanh') %>%
layer_dense(units = 1,
activation = 'exponential',
use_bias = TRUE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
nn_freq_ageph %>%
fit(x = ageph, #<<
y = counts / exposure,
sample_weight = exposure,
epochs = 20,
batch_size = 1024,
validation_split = 0,
verbose = 0)
library(mgcv)
gam_ageph <- gam(nclaims ~ s(ageph),
data = mtpl_train,
family = poisson(link = 'log'),
offset = log(expo))
df_age <- tibble::tibble(
age = 18:95,
NN = as.numeric(predict(nn_freq_ageph, age)),
GAM = predict(gam_ageph, type = 'response',
newdata = data.frame(ageph = age))
)
ggplot(df_age, aes(x = age)) + geom_line(aes(y = GAM)) + geom_point(aes(y = NN)) +
theme_bw() + labs(y = 'Fitted effect') + ggtitle('NN (dots) vs. GAM (line)')
nn_freq_ageph <-
keras_model_sequential() %>%
layer_batch_normalization(input_shape = c(1)) %>%
layer_dense(units = 5,
activation = 'tanh') %>%
layer_dense(units = 1,
activation = 'exponential',
use_bias = TRUE) %>%
compile(loss = 'poisson',
optimize = optimizer_rmsprop())
nn_freq_ageph %>%
fit(x = ageph, #<<
y = counts / exposure,
sample_weight = exposure,
epochs = 30,
batch_size = 1024,
validation_split = 0,
verbose = 0)
df_age <- tibble::tibble(
age = 18:95,
NN = as.numeric(predict(nn_freq_ageph, age)),
GAM = predict(gam_ageph, type = 'response',
newdata = data.frame(ageph = age))
)
ggplot(df_age, aes(x = age)) + geom_line(aes(y = GAM)) + geom_point(aes(y = NN)) +
theme_bw() + labs(y = 'Fitted effect') + ggtitle('NN (dots) vs. GAM (line)')
nn_freq_ageph %>% keras::save_model_tf('nn_freq_ageph')
input_skip <- layer_input(shape = c(1), name = 'skip')
input_nn <- layer_input(shape = c(1), name = 'nn')
network <- input_nn %>% layer_batch_normalization() %>% layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 1, activation = `'linear'`)
input_skip <- layer_input(shape = c(1), name = 'skip')
input_nn <- layer_input(shape = c(1), name = 'nn')
network <- input_nn %>% layer_batch_normalization() %>% layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 1, activation = 'linear')
output <- list(network, input_skip) %>% layer_add() %>%
layer_dense(units = 1, activation = 'exponential', trainable = FALSE, name = 'output',
weights = list(array(1, dim = c(1,1)), array(0, dim = c(1))))
cann <- keras_model(inputs = list(input_nn, input_skip), outputs = output)
cann %>% compile(loss = 'poisson', optimize = optimizer_rmsprop())
cann %>% deepviz::plot_model()
gam_expo <- predict(gam_ageph) + log(mtpl_train$expo)
cann_input <- list('nn' = mtpl_train$ageph,
'skip' = gam_expo)
cann %>% fit(x = cann_input,
y = counts,
epochs = 20,
batch_size = 1024,
validation_split = 0,
verbose = 0)
df <- tibble::tibble(ageph = 18:95,
skip = 0)
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df) + theme_bw() + geom_point(aes(ageph, effect)) + ggtitle('NN adjustments')
df <- tibble::tibble(ageph = 18:95,
skip = predict(gam_ageph, newdata = as.data.frame(ageph)))
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df, aes(x = ageph)) + geom_point(aes(y = effect)) + geom_line(aes(y = exp(skip))) +
theme_bw() + ggtitle('CANN (dots) vs. GAM (line)')
input_skip <- layer_input(shape = c(1), name = 'skip')
input_nn <- layer_input(shape = c(1), name = 'nn')
network <- input_nn %>% layer_batch_normalization() %>% layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 1, activation = 'linear')
output <- list(network, input_skip) %>% layer_add() %>%
layer_dense(units = 1, activation = 'exponential', trainable = FALSE, name = 'output',
weights = list(array(1, dim = c(1,1)), array(0, dim = c(1))))
cann <- keras_model(inputs = list(input_nn, input_skip), outputs = output)
cann %>% compile(loss = 'poisson', optimize = optimizer_rmsprop())
cann %>% fit(x = cann_input,
y = counts,
epochs = 20,
batch_size = 1024,
validation_split = 0,
verbose = 0)
df <- tibble::tibble(ageph = 18:95,
skip = 0)
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df) + theme_bw() + geom_point(aes(ageph, effect)) + ggtitle('NN adjustments')
df <- tibble::tibble(ageph = 18:95,
skip = predict(gam_ageph, newdata = as.data.frame(ageph)))
df <- df %>% dplyr::mutate(effect = cann %>% predict(list(df$ageph, df$skip)))
ggplot(df, aes(x = ageph)) + geom_point(aes(y = effect)) + geom_line(aes(y = exp(skip))) +
theme_bw() + ggtitle('CANN (dots) vs. GAM (line)')
cann %>% keras::save_model_tf('cann_freq_ageph')
nn_sev_log <- keras_model_sequential() %>%
layer_dense(units = 1, activation = 'linear',
input_shape = c(1), use_bias = FALSE) %>%
compile(loss = 'mse', #<<
optimize = optimizer_rmsprop())
claims <- mtpl_train %>% dplyr::filter(nclaims > 0)
nn_sev_log %>%
fit(x = rep(1, nrow(claims)),
y = log(claims$avg), #<<
epochs = 100, batch_size = 128,
validation_split = 0, verbose = 0)
predict(nn_sev_log, 1) %>% exp() %>% as.numeric()
claims$avg %>% summary()
library(recipes)
# Create and prepare the recipe
mtpl_recipe <- recipe(nclaims ~ ., data = mtpl_train) %>%
step_rm(id, amount, avg, town, pc) %>%
step_nzv(all_predictors(), -expo) %>%
step_normalize(all_numeric(), -c(nclaims, expo)) %>%
step_dummy(all_nominal(), one_hot = TRUE) %>%
prep(mtpl_train)
# Bake the training and test data
mtpl_train_b <- mtpl_recipe %>% juice()
mtpl_test_b <- mtpl_recipe %>% bake(new_data = mtpl_test)
# Make the data NN proof
train_x <- mtpl_train_b %>%
dplyr::select(-c(nclaims, expo)) %>% as.matrix()
test_x <- mtpl_test_b %>%
dplyr::select(-c(nclaims, expo)) %>% as.matrix()
train_y <- mtpl_train_b$nclaims
test_y <- mtpl_test_b$nclaims
train_expo <- mtpl_train_b$expo
test_expo <- mtpl_test_b$expo
nn_case <- keras_model_sequential() %>%
layer_dense(units = 20,
activation = 'relu',
input_shape = ncol(train_x)) %>%
layer_dense(units = 10,
activation = 'relu') %>%
layer_dense(units = 1,
activation = 'exponential') %>%
compile(loss = 'poisson',
optimize = optimizer_nadam())
nn_case %>%
fit(x = train_x,
y = train_y / train_expo,
sample_weight = train_expo,
epochs = 20,
batch_size = 1024,
validation_split = 0,
verbose = 0)
# Built-in evaluation
nn_case %>%
evaluate(x = test_x,
y = test_y,
verbose = 0)
# If you want to check the results
poisson_loss <- function(pred, actual) {
mean(pred - actual * log(pred))
}
poisson_loss(predict(nn_case, test_x),
test_y)
# Use array for weights in evaluate
nn_case %>%
evaluate(x = test_x,
y = test_y,
sample_weight = array(test_expo),
verbose = 0)
gam_case <- gam(
nclaims ~ coverage + fuel + sex +
s(ageph) + s(bm) + s(power) + s(agec),
data = mtpl_train,
offset = log(expo),
family = poisson(link = 'log')
)
poisson_loss(predict(gam_case, mtpl_test,
type = 'response'),
test_y)
nn_case %>% keras::save_model_tf('nn_case')
