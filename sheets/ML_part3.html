<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Hands-on Machine Learning with R - Module 3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katrien Antonio &amp; Roel Henckaerts" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/viz/viz.js"></script>
    <link href="libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
    <script src="libs/grViz-binding/grViz.js"></script>
    <link rel="stylesheet" href="css/metropolis.css" type="text/css" />
    <link rel="stylesheet" href="css/metropolis-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Hands-on Machine Learning with R - Module 3
## Hands-on webinar
<html>
<div style="float:left">

</div>
<hr align='center' color='#116E8A' size=1px width=97%>
</html>
### Katrien Antonio &amp; Roel Henckaerts
### <a href="https://github.com/katrienantonio/hands-on-machine-learning-R-module-3">hands-on-machine-learning-R-module-3</a> | January 28 &amp; February 4, 2021

---








class: inverse, center, middle
name: prologue

# Prologue

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

name: introduction

# Introduction

### Course

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; https://github.com/katrienantonio/hands-on-machine-learning-R-module-3

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt; [https://katrienantonio.github.io/](https://katrienantonio.github.io/) &amp; [https://henckr.github.io/](https://henckr.github.io/)

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/&gt;&lt;/svg&gt; [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) &amp; [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 640 512"&gt;&lt;path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/&gt;&lt;/svg&gt; (Katrien) Professor in insurance data science

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 640 512"&gt;&lt;path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/&gt;&lt;/svg&gt; (Roel) PhD student in insurance data science

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  
  ```r
  version$version.string
  ## [1] "R version 4.0.3 (2020-10-10)"
  ```

☑ Do you have a fairly recent version of RStudio? 
  
  ```r
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```

☑ Have you installed the R packages listed in the software requirements? 

or

☑ Have you created an account on RStudio Cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this module .font140[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 512 512"&gt;&lt;path d="M505.05 19.1a15.89 15.89 0 0 0-12.2-12.2C460.65 0 435.46 0 410.36 0c-103.2 0-165.1 55.2-211.29 128H94.87A48 48 0 0 0 52 154.49l-49.42 98.8A24 24 0 0 0 24.07 288h103.77l-22.47 22.47a32 32 0 0 0 0 45.25l50.9 50.91a32 32 0 0 0 45.26 0L224 384.16V488a24 24 0 0 0 34.7 21.49l98.7-49.39a47.91 47.91 0 0 0 26.5-42.9V312.79c72.59-46.3 128-108.4 128-211.09.1-25.2.1-50.4-6.85-82.6zM384 168a40 40 0 1 1 40-40 40 40 0 0 1-40 40z"/&gt;&lt;/svg&gt;]

--

* .KULbginline[de-mystify] neural networks

--

* develop foundations of working with (different types of) .KULbginline[neural networks]

--

* focus on the use of neural networks for the .KULbginline[analysis of claim frequency + severity data], also in combination with GLMs or tree-based ML models 

--

* discuss how to .KULbginline[evaluate] and .KULbginline[interpret] neural networks

--

* step from simple networks (for regression) to .KULbginline[auto encoders] and .KULbginline[convolutional] networks.

--



---

# Want to read more?

.pull-left[

This presentation is based on

* Michael A. Nielsen (2015) [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/)

* the work of prof. Taylor Arnold, in particular Chapter 8 in the book [A computational approach to statistical learning](https://www.routledge.com/A-Computational-Approach-to-Statistical-Learning/Arnold-Kane-Lewis/p/book/9780367570613) by Arnold, Kane &amp; Lewis (2019)

* Boehmke (2020) on [Deep Learning with R: using Keras with TensorFlow backend](https://github.com/rstudio-conf-2020/dl-keras-tf).   

]

.pull-right[

Actuarial modelling with neural nets is covered in

* Wüthrich &amp; Buser (2020) [Data analytics for non-life insurance pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308), in particular Chapter 5

* Wüthrich (2019) [From Generalized Linear Models to neural networks, and back](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3491790)

* Wüthrich &amp; Merz (2019) [Editorial: Yes, we CANN!](https://www.cambridge.org/core/journals/astin-bulletin-journal-of-the-iaa/article/editorial-yes-we-cann/66E8BEC373B5CCEF3BF3303D442D6B75), in ASTIN Bulletin 49/1.

]

---

# Module 3's Outline

.pull-left[

* [Getting started](#start)
  - Unpacking our toolbox
  - Tensors

* [De-mystifying neural networks](#demystify)
  - What's in a name?
  - A simple neural network
  
* [Neural network architecture](#fundamentals)
  - An architecture with layers in {keras}
  
* [Network compilation](#compilation)
  - Loss function and forward pass
  - Gradient descent and backpropagation
  - Performance metrics
  - Model evaluation
  


]

.pull-right[  

* [Regression with neural networks](#regression)
  - Redefining GLMs as a neural network
  - Including exposure
  - Case study

* [Auto encoders](#autoencoder)
  - Data compression and feature extraction
  - Evaluation

* [Convolutional neural networks](#cnn)
  - Handling new data formats
  - Convolutional layers explained
  - Evaluation and intepretation
  
]



---

name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

&lt;img src = "img/AI_ML_DL.jpg" height = "350px" /&gt;

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---
class: inverse, center, middle
name: start

# Getting started

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# The programming framework for today

&lt;br&gt;
&lt;img src="img/modelflow.png" width="600" height="100" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

* R: &lt;br&gt;Our favorite programming language, including an R interface to Keras and TensorFlow. 

* Keras: &lt;br&gt;An inuitive high level Python interface to TensorFlow.

* TensorFlow: &lt;br&gt; Open source platform for machine learning developed by the Google Brain Team, see [https://www.tensorflow.org/](https://www.tensorflow.org/). &lt;br&gt; Special focus on training deep neural networks.


---

# R packages

Today's session will make extensive use of {keras} and {tidyverse}. &lt;br&gt;
Do not forget to load these packages in your R session.


```r
library(keras)
library(tidyverse)
```

.pull-left[
.center[
&lt;img src = "img/keras.jpg" height="200px" /&gt;
]
]

.pull-right[
.center[
&lt;img src = "img/tidyverse.png" height="200px" /&gt;
]
]


---

# Why is this thing called TensorFlow?

A scalar is a single number, or a 0D tensor, i.e. .hi-pink[zero dimensional]:

&lt;!-- `$$\text{age_car = 5}, \quad \text{fuel = gasoline}, \quad \text{bm = 10}$$` --&gt;
.center[
`age_car = 5`, &amp;nbsp;&amp;nbsp; `fuel = gasoline`, &amp;nbsp;&amp;nbsp; `bm = 10`
]

In tensor parlance a scalar has 0 axes.

--

In a .hi-pink[big data world] with structured and unstructured data, our .hi-pink[input] can be a

* a single time series: 1-dimensional, with 1 axis

* one sound fragment: 2-dimensional, with 2 axes

* one image in color: 3-dimensional, with 3 axes

* one movie: 4-dimensional, with 4 axes

* ...

--

We require a framework that can flexibly adjust to all these data structures!

---

# Why is this thing called TensorFlow? (cont.)

.hi-pink[TensorFlow] is this flexible framework which consists of highly optimized functions based on .hi-pink[tensors].

What is a .hi-pink[tensor]?

* A 1-dimensional tensor is a vector (e.g. closing daily stock price during 250 days)

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-5-1.png" width="300" height="20" style="display: block; margin: auto;" /&gt;

--
* A 2-dimensional tensor is a matrix (e.g. a tabular data set with observations and features)

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-6-1.png" width="300" height="100" style="display: block; margin: auto;" /&gt;

* ...
--
Tensors generalize vectors and matrices to an arbitrary number of dimensions. 

Many matrix operations, such as the matrix product, can be generalized to tensors.

Luckily Keras provides a high level interface to TensorFlow, such that we will have only minimal exposure to tensors and the complicated math behind them.


---

# Example of a 3D tensor

.pull-left[

Let's picture a stock price dataset where

* each minute we record the current price, lowest price and highest price
* a trading day has 390 minutes and a trading year has 250 days.

Then, one year of data can then be stored in a 3D tensor `(samples, timesteps, features)`, here: `(250, 390, 3)`.

]

.pull-right[

.center[
&lt;img src = "img/3D_tensor.png" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Example of a 4D tensor

.pull-left[

Let's picture an image data set where

* each image has a specific height and width
* three color channels (Red, Green, Blue) are registered
* multiple images (`samples`) are stored.


Then, a collection of images can be stored in a 4D tensor `(samples, height, width, channels)`.

]

.pull-right[

.center[
&lt;img src = "img/4D_tensor.png" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Example of a 5D tensor

.pull-left[

Let's picture an video data set where

* each video sample is one minute long and has a number of frames per second (e.g. 4 frames per second)
* each frame has a specific height (e.g. 256 pixels) and width (e.g. 144 pixels)
* three color channels (Red, Green, Blue)
* multiple images (`samples`) are stored.


Then, a collection of images can be stored in a 4D tensor `(samples, frames, height, width, channels)` which becomes here `(samples, 240, 256, 144, 3)`.


]

.pull-right[

.center[
&lt;img src = "img/5D_tensor.jpg" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Tensor functions

.pull-left[
Keras generalizes common R functions for inputs of type tensor. These functions can be recognized by the prefix `k_`.

See the [Keras documentation](https://keras.rstudio.com/articles/backend.html#backend-functions) for a list of all tensor functions.

* `k_constant`: create and initialise a tensor. 

```r
x &lt;- k_constant(c(1, 2, 3, 4, 5, 6), 
                shape = c(3, 2))
x
## tf.Tensor(
## [[1. 2.]
##  [3. 4.]
##  [5. 6.]], shape=(3, 2), dtype=float32)
```
]

.pull-right[
Most tensor operations require an axis parameter to specify the dimensions over which the function should be performed.

* `k_mean`: calculate the mean of the tensor.


```r
k_mean(x, axis = 1)
## tf.Tensor([3. 4.], shape=(2,), dtype=float32)
```


```r
k_mean(x, axis = 2)
## tf.Tensor([1.5 3.5 5.5], shape=(3,), dtype=float32)
```

]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

&lt;br&gt;
In this warming up exercise you .KULbginline[create a tensor] and .KULbginline[apply basic tensor functions].
&lt;br&gt;
&lt;br&gt;
* .hi-pink[Q.1]: create a 3-dimensional tensor in R with values `1, 2, ..., 12` and shape `(2, 3, 2)`.

* .hi-pink[Q.2]: calculate the logarithm of this tensor.

* .hi-pink[Q.3]: calculate the mean of this tensor over the third axis. 
]

---

class: clear

.pull-left[
.hi-pink[Q.1]: create a tensor

```r
x &lt;- k_constant(1:12, shape = c(2, 3, 2))
x
## tf.Tensor(
## [[[ 1.  2.]
##   [ 3.  4.]
##   [ 5.  6.]]
## 
##  [[ 7.  8.]
##   [ 9. 10.]
##   [11. 12.]]], shape=(2, 3, 2), dtype=float32)
```

.hi-pink[Q.2]: calculate the logarithm 

```r
k_log(x)
## tf.Tensor(
## [[[0.        0.6931472]
##   [1.0986123 1.3862944]
##   [1.609438  1.7917595]]
## 
##  [[1.9459102 2.0794415]
##   [2.1972246 2.3025851]
##   [2.3978953 2.4849067]]], shape=(2, 3, 2), dtype=float32)
```
]

.pull-right[
`log(x)` would have also resulted in the correct answer.&lt;br&gt;However, it is best practice to use `k_log`, since the R-function `log` can not be evaluated within TensorFlow:

```r
library(tensorflow)
tf$\`function`(k_log)(x)
```

```
## tf.Tensor(
## [[[0.        0.6931472]
##   [1.0986123 1.3862944]
##   [1.609438  1.7917595]]
## 
##  [[1.9459102 2.0794415]
##   [2.1972246 2.3025851]
##   [2.3978953 2.4849067]]], shape=(2, 3, 2), dtype=float32)
```


```r
tf$\`function`(log)(x)
```

```
## Error in py_call_impl(...) : 
##  Unable to convert R object to Python type
```

]

---

class: clear

.pull-left[
.hi-pink[Q.3]: calculate the mean along the third axis

```r
k_mean(x, axis = 3)
## tf.Tensor(
## [[ 1.5  3.5  5.5]
##  [ 7.5  9.5 11.5]], shape=(2, 3), dtype=float32)
```
... and explore the other axes as well
]

.pull-right[
.hi-pink[Q.3]: mean along the first axis

```r
k_mean(x, axis = 1)
## tf.Tensor(
## [[4. 5.]
##  [6. 7.]
##  [8. 9.]], shape=(3, 2), dtype=float32)
```

or the second axis

```r
k_mean(x, axis = 2)
## tf.Tensor(
## [[ 3.  4.]
##  [ 9. 10.]], shape=(2, 2), dtype=float32)
```
]

---

class: inverse, center, middle
name: data-sets

# Data sets used in the course 

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

name: data-sets-used

# Data sets used in this course - MTPL &lt;img src="img/pipe.png" class="title-hex"&gt; &lt;img src="img/dplyr.png" class="title-hex"&gt; &lt;img src="img/ggplot2.png" class="title-hex"&gt;

We will (once again) use the Motor Third Party Liability data set. There are 163,231 policyholders in this data set. 

The frequency of claiming (`nclaims`) and corresponding severity (`avg`, the amount paid on average per claim reported by a policyholder) are the .KULbginline[target variables] in this data set. 

Predictor variables are: 

* the exposure-to-risk, the duration of the insurance coverage (max. 1 year)
* factor variables, e.g. gender, coverage, fuel
* continuous, numeric variables, e.g. age of the policyholder, age of the car
* spatial information: postal code (in Belgium) of the municipality where the policyholder resides.

More details in [Henckaerts et al. (2018, Scandinavian Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#data-driven) and [Henckaerts et al. (2019, arxiv)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#tree-based-pricing).

---

class: clear, center, middle

background-image: url("img/MnistExamples.png")
background-size: cover
background-size: 95% 
background-position: left

.font1000.bold[MNIST]

---

name: data-sets-used

# Data sets used in this course - MNIST

As discussed, not all data are in tabular format. 

We analyze an .KULbginline[image database] from the Modified National Institute of Standards and Technology, short [MNIST](https://en.wikipedia.org/wiki/MNIST_database). 

Working with MNIST will learn us how machine learning methods can be used to work with new data sources, such as images. 

.pull-left[
* Large database of 70,000 labeled images of handwritten digits, see [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)

* Images are preprocessed, i.e. scaled and centered.

* Classic test case for machine learning classification algorithms. Current models achieve an accuracy of [more than 99.5%](https://en.wikipedia.org/wiki/MNIST_database). 
]

.pull-right[
.center[
&lt;img src = "img/neural_network_sample.gif" height = "350px" /&gt;
]
]


---

# Data sets used in this course - MNIST




.pull-left[

The images are in grayscale. Each image is stored as a 28x28 intensity matrix, with intensity expressed on a scale from 0-255.

&lt;img src="ML_part3_files/figure-html/example_mnist_input-1.png" width="360" height="360" style="display: block; margin: auto;" /&gt;
]

.pull-right[
Recognizing that the images below all represent the digit 8 is trivial for humans, but difficult for computers.

&lt;img src="ML_part3_files/figure-html/plot_example_mnist-1.svg" height="150" style="display: block; margin: auto;" /&gt;

Neural networks are ideal for situations where the relation between the input (here: intensity matrix) and the output (here: 0-9) is complicated.
]

---
# Loading the MNIST dataset

.pull-left[

`keras::dataset_mnist()` retrieves the MNIST dataset from the online repository. &lt;br&gt; Alternatively, the dataset can be loaded from the course directory.

```r
# download the dataset from the online repository
mnist &lt;- keras::dataset_mnist()
# load the dataset from the course files
load('../data/mnist.RData')
```

Assign new names to the input and output data.

```r
input &lt;- mnist$train$x
output &lt;- mnist$train$y
test_input &lt;- mnist$test$x
test_output &lt;- mnist$test$y
```
]

.pull-right[
The input data is an `array`:

```r
class(input)
```


```
## [1] "array"
```

with 60,000 28x28 images (the training data):

```r
dim(input)
```


```
## [1] 60000    28    28
```

Select the first image:

```r
input[1, , ]
```
]

---

class: inverse, center, middle
name: demystify

# De-mystifying neural networks

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# What's in a name?

.pull-left[
Different types of neural networks and their applications: 

* .KULbginline[ANN]: Artificial Neural Network &lt;br&gt; for regression and classification problems, with vectors as input data

* .KULbginline[CNN]: Convolutional Neural Network &lt;br&gt; for image processing, image/face/... recognition, with images as input data

* .KULbginline[RNN]: Recurrent Neural Network &lt;br&gt; for sequential data such as text or time series

... and many more!
]

.pull-right[

&lt;div style='height:100%; overflow:scroll;'&gt;
&lt;img src = "img/neural_network_types.png" /&gt;
&lt;/div&gt;

]
---

# A simple neural network

.pull-left[

De-mystify artificial neural networks (ANNs): 

* a collection of inter-woven linear models
* extending linear approaches to detect .KULbginline[non-linear] interactions in .KULbginline[high-dimensional] data.

See the picture on the right.

.KULbginline[Goal]: predict a scalar response `\(y\)` from scalar input `\(x\)`.

]

.pull-right[



&lt;img src = "img/a_simple_neural_network.png" /&gt;

Some terminology: 

* `\(x\)` is the .KULbginline[input layer]
* `\(v\)` is the .KULbginline[output layer]
* middle layer is a .KULbginline[hidden layer]
* four neurons: `\(x\)`, `\(z_1\)`, `\(z_2\)` and `\(v\)`.


]

---

# A simple neural network (cont.)

.pull-left[

First, we apply two independent .KULbginline[linear models]:
$$
`\begin{eqnarray*}
z_1 &amp;=&amp; b_1 + x \cdot w_1 \\
z_2 &amp;=&amp; b_2 + x \cdot w_2
\end{eqnarray*}`
$$
using four parameters: two intercepts and two slopes.

Next, we construct .KULbginline[another linear model] with the `\(z_j\)` as inputs:
$$
`\begin{eqnarray*}
\hat{y} := v &amp;=&amp; b_3 + z_1 \cdot u_1 + z_2 \cdot u_2.
\end{eqnarray*}`
$$
Putting it all together:
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 +  z_1 \cdot u_1 + z_2 \cdot u_2 \\
&amp;=&amp; b_3 + (b_1 + x \cdot w_1)\cdot u_1 + (b_2+ x \cdot w_2) \cdot u_2 \\
&amp;=&amp; (b_3 + u_1 \cdot b_1+ u_2 \cdot b_2) + (w_1 \cdot u_1 + w_2 \cdot u_2) \cdot x \\
&amp;=&amp; \text{(intercept)} + \text{(slope)} \cdot x. 
\end{eqnarray*}`
$$

]


.pull-right[

Model is over-parametrized, with infinitely many ways to describe the same model. 

Essentially, still a linear model!

&lt;img src = "img/sum_linear_models.png" /&gt;

]

---

# A simple neural network (cont.)

.pull-left[
We capture .KULbginline[non-linear] relationships between `\(x\)` and `\(v\)` by replacing
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 + z_1 \cdot u_1 + z_2 \cdot u_2.
\end{eqnarray*}`
$$
with
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 + \sigma(z_1) \cdot u_1 + \sigma(z_2) \cdot u_2 \\
&amp;=&amp; b_3 + \sigma(b_1 + x \cdot w_1) \cdot u_1 + \sigma(b_2 + x \cdot w_2) \cdot u_2,
\end{eqnarray*}`
$$
where `\(\sigma(.)\)` is an .KULbginline[activation function], a mapping from `\(\mathbb{R}\)` to `\(\mathbb{R}\)`.


Adding an activation function greatly increases the .KULbginline[set of possible relations] between `\(x\)` and `\(v\)`!
]

.pull-right[
For example, the rectified linear unit (ReLU) activation function:
$$
`\begin{eqnarray*}
\text{ReLU}(x) &amp;=&amp; \begin{cases} \begin{array}{cc}
                                   x, &amp; \text{if}\ x \geq 0 \\
                                   0, &amp; \text{otherwise}.
                                 \end{array}
                    \end{cases}
\end{eqnarray*}`
$$

&lt;img src = "img/sum_ReLu.png" /&gt;

Many more activation functions: sigmoid, softmax, identity, etc. (see further).
]

---

# From the simple neural network to ANNs

.pull-left[
Artificial Neural networks (.KULbginline[ANNs]): 

* a collection of neurons 
* organized into an ordered set of layers 
* directed connections pass signals between neurons in adjacent layers 
* .hi-pink[to train]: &lt;br&gt; update parameters describing the connections by minimizing loss function over training data 
* .hi-pink[to predict]: &lt;br&gt; pass `\(\boldsymbol{x}_i\)` to first layer, output of final layer is `\(\hat{y}_i\)`.

The network is .KULbginline[dense] or .KULbginline[densely connected] if each neuron in a layer receives an input from all the neurons present in the previous layer.



]

.pull-right[

&lt;img src = "img/feed_forward.png"/&gt;

This is a .KULbginline[feedforward] neural network - no loops!

]

---

# The neural nets' terminology

.pull-left[
Using the neural nets terminology or language: 

* intercept called .KULbginline[the bias]

* slopes called .KULbginline[weights] 

* `\(L+1\)` layers in total, with input layer denoted as layer 0 and output layer as `\(L\)`

* use `\(a\)` (from .KULbginline[activation]) to denote the output of a given neuron in a given layer. 

]

.pull-right[

&lt;img src = "img/perceptron.png"/&gt;


A single layer ANN, also called perceptron or artificial neuron.

]

---

class: inverse, center, middle
name: fundamentals

# Neural network architecture in Keras

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# Preparing the MNIST data for an ANN

.pull-left[

Let's construct a densely connected, feed forward neural network for the multi-class classification problem in MNIST.

We .KULbginline[flatten] the image data (28x28 matrix) into a vector of length 784:


```r
input &lt;- tensorflow::array_reshape(input,
                    c(nrow(input), 28*28)) / 255

test_input &lt;- tensorflow::array_reshape(test_input,
                    c(nrow(test_input), 28*28)) / 255
```

Later in this course we will see how we can analyze this data .hi-pink[without flattening]!

]

.pull-right[

We construct 10 dummy variables (0-9) for the output of the model:


```r
output &lt;- keras::to_categorical(output, 10)
```



```r
test_output &lt;- keras::to_categorical(test_output, 10)
```




]

---

# Preparing the MNIST data for an ANN (cont.)

.pull-left[

The R-script includes a function `plot_image` to visualize the input:


```r
plot_image(input[17, ], legend = FALSE)
```

]

.pull-right[

&lt;img src="ML_part3_files/figure-html/plot_image_function-1.png" style="display: block; margin: auto;" /&gt;

]

---

# An architecture with layers

In a neural network, **.green[input]** travels through a sequence of **.blue[layers]**, 
and gets transformed into the **.red[output]**.

.pull-left[
&lt;img src="img/example_neural_network.png" width="400" height="250" style="display: block; margin: auto;" /&gt;
]

.pull-right[
This sequential layer structure is really at the core of the Keras libary.


```r
model &lt;- 
* keras_model_sequential() %&gt;%
  layer_dense(...) %&gt;%
  layer_dense(...)
```
]

&lt;br&gt;

.KULbginline[Layers] consist of .KUlbginline[nodes] and the .KULbginline[connections] between these nodes and the previous layer.

`layer_dense()` is creating a fully connected feed forward neural network.

---

# An architecture with layers (cont.)

.pull-left[


```r
model &lt;- keras_model_sequential() %&gt;%
* layer_dense() %&gt;% # hidden layer
* layer_dense() # output layer
```

Each `layer_dense()` represents a hidden layer *or* the final output layer.


```r
model &lt;- keras_model_sequential() %&gt;%
* layer_dense() %&gt;% # hidden layer 1
* layer_dense() %&gt;% # hidden layer 2
* layer_dense() %&gt;% # hidden layer 3
* layer_dense() # output layer
```

]

.pull-right[

* We can add multiple hidden layers by adding more `layer_dense()` functions.

* Technically, .KULbginline[deep learning] refers to any neural network that has 
2 or more hidden layers.

* The last `layer_dense()` will always represent the output layer.

]

---

# A hidden layer


```r
model &lt;- keras_model_sequential() %&gt;%
*        layer_dense(units = 512, activation = 'relu', input_shape = c(784)) # hidden layer
```

.pull-left[

* `units = 512`: number of nodes in the given layer

* `input_shape = c(784)`
   - tells the first hidden layer how many input features there are
   - only required for the first `layer_dense`
   
* `activation = 'relu'`: this hidden layer uses the ReLU activation function. 

Here: the MNIST pictures (28x28) are flattened to a an input vector of length 784.


]

.pull-right[

&lt;img src="img/hidden_layer.png" style="display: block; margin: auto;" /&gt;

]

---

# A hidden layer - some intuition

Nodes in the hidden layer(s) represent intermediary features that we do not explicitely define. 

We let the model decide the optimal features.

For example, recognizing a digit is more difficult than recognizing a horizontal or vertical line.

&lt;img src="img/neural_network_digital.png" width="700" height="100" style="display: block; margin: auto;" /&gt;

Hidden layers automatically split the problem into smaller problems that are easier to model.

&lt;img src="img/neural_network_digital_example.png" width="700" height="100" style="display: block; margin: auto;" /&gt;

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`:


]

.pull-right[

&lt;img src="img/output_layer_continuous.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`
   - binary classification: `units = 1`
   
]

.pull-right[




&lt;img src="img/output_layer_binary.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`
   - binary classification: `units = 1`
   - multi-class classification: `units = n`
]
.pull-right[

&lt;img src="img/output_layer_multi.png" width="75%" height="75%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
]
.pull-right[

&lt;img src="img/activation_identity.png" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
   - binary classification: `activation = 'sigmoid'`
]
.pull-right[
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-48-1.png" style="display: block; margin: auto;" /&gt;

`\(f(y) = \frac{1}{1 + e^{-y}}\)`
]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
   - binary classification: `activation = 'sigmoid'`
   - multi-class classification: `activation = 'softmax'`

]

.pull-right[

&lt;img src="img/softmax.png" style="display: block; margin: auto;" /&gt;

]


---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

&lt;br&gt;
Ultimately, here is a summary of the network architecture discussed so far for the MNIST data


```r
model &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 512, 
              activation = 'relu', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 10, 
              activation = 'softmax')
  
```

Can you figure out how many parameters will be trained for this network?

]

---

class: clear

.pull-left[


```
## Model: "sequential"
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## dense_1 (Dense)                     (None, 512)                     401920      
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      5130        
## ================================================================================
## Total params: 407,050
## Trainable params: 407,050
## Non-trainable params: 0
## ________________________________________________________________________________
```

]

.pull-right[

The model has 407,050 parameters:

* 784 inputs (28x28 pixels in a single image) 

* 1 hidden layer, with
  - 512 nodes and ReLU activation
  - thus, (784 x 512) + 512 = 401,920 parameters

* multi-class output layer, with
  - 10 nodes
  - softmax activation function
  - thus, (512 x 10) + 10 = 5,130 parameters
  
* all together, that makes 407,050 parameters!

]


---


class: inverse, center, middle
name: compilation

# Network compilation

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# Loss function and forward pass

.pull-left[

* Initialize weights (randomly).

* The forward pass then results in predicted values `\(\hat{\textbf{y}}\)`, to be compared with `\(\textbf{y}\)`.

* The difference is measured with a loss function, the quantity that will be minimized during training.

Keras includes many .KULbginline[common loss functions]:

* `"mse"`: Gaussian
* `"poisson"`: Poisson
* `"binary_crossentropy"`: binary classification
* `"categorical_crossentropy"`: multi-class classification
* many others, see the [Keras documentation](https://keras.io/losses/)

Pick a loss function that aligns best to the problem at hand!
]

.pull-right[

&lt;img src="img/forward_pass3.png" style="display: block; margin: auto;" /&gt;

]

---

# Compiling the model

.pull-left[

```r
model &lt;- model %&gt;%
* compile(loss = "categorical_crossentropy",
          optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```


With `loss = "categorical_crossentropy"` the loss of a single training observation is 

$$ \sum_{j=0}^9 -p_j \cdot \log{(f(y_j))},   $$
where `\(j\)` runs over the classes in the multi-class prediction problem, `\(f(y_j)\)` is the fitted probability of class `\(j\)` and `\(p_j\)` is a 0/1 hot-encoding of the truly observed class.

For instance, when the true input digit is 1 the vector `\(p\)` is `\((0, 1, 0, 0, \ldots, 0)\)`.

]

.pull-right[

You can also define your  .hi-pink[own loss] function in Keras, e.g.

```r
mse &lt;- function(y_true, y_pred) { 
  k_mean((y_true - y_pred)^2, axis = 2) 
} 

model &lt;- model %&gt;%
* compile(loss = mse,
          optimizer = optimizer_rmsprop(),
          metrics = c('accuracy'))
```

* `k_mean` is the keras implementation of `mean` that takes a tensor as input.

* `axis = 2` calculates the mean over the different output nodes.
]

---
# Compiling the model (cont.)

.pull-left[

```r
model &lt;- model %&gt;%
  compile(loss = "categorical_crossentropy", 
*         optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```


Keras includes several .KULbginline[optimizers] for minimizing the loss function. 

Popular choices are:
* `optimizer_rmsprop()`
* `optimizer_adam()`
* other optimizers, see the [Keras documentation](https://keras.io/optimizers/)

]

.pull-right[

The goal is to find weights and bias terms that .KULbginline[minimize the loss function].

&lt;img src="img/forward_pass4.png" style="display: block; margin: auto;" /&gt;


]

---

# Gradient descent and backpropagation

.pull-left[

In general terms, we want to find (with `\(w\)` for all unknown parameters)

`$$\min_{w} \mathcal{L}(w),$$`
With .KULbginline[gradient descent]: we'll move in the direction the loss locally decreases the fastest!

Thus, 

`$$w_{\text{new}} = w_{\text{old}} - \eta \cdot \nabla_{w} \mathcal{L}(w_{\text{old}}),$$`

with learning rate `\(\eta\)`. 

With a loss function evaluated over `\(n\)` training data points (cfr. supra on *epochs* and *minibatches*)

`$$\nabla_w \mathcal{L}(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \mathcal{L}_i$$`.

]

---

# Gradient descent and backpropagation

.pull-left[

In general terms, we want to find (with `\(w\)` for all unknown parameters)

`$$\min_{w} \mathcal{L}(w),$$`
With gradient descent: we'll move in the direction the loss locally decreases the fastest!

Thus, 

`$$w_{\text{new}} = w_{\text{old}} - \eta \cdot \nabla_{w} \mathcal{L}(w_{\text{old}}),$$`

with learning rate `\(\eta\)`. 

With a loss function evaluated over `\(n\)` training data points (cfr. supra on *epochs* and *minibatches*)

`$$\nabla_w \mathcal{L}(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \mathcal{L}_i$$`.

]

.pull-right[

Computing the gradient of the loss function wrt all trainable parameters:

* tons of parameters
* need for efficient algorithm to calculate gradient
* need for generic algorithm usable for arbitrary number of layers and neurons in each layer.

The strategy (Rumelhart et al., 1986, Nature)

* [backpropagation](https://en.wikipedia.org/wiki/Backpropagation)
* derivatives in outer layer L are easy 
* derivatives in layer `\(l\)` as a function of derivatives in layer `\(l+1\)`
* all about the .KULbginline[chain rule] for derivatives!



]


---

# Performance metrics

.pull-left[

```r
model &lt;- model %&gt;%
  compile(loss = "categorical_crossentropy", 
          optimize = optimizer_rmsprop(), 
*         metrics = c('accuracy'))
```


In additition to the loss function, other .KULbginline[performance measures (metrics)] can be tracked while calibrating the model.

* `accuracy` (= categorical_accuracy)
* `binary_accuracy`
* `categorical_accuracy`
* `sparse_categorical_accuracy`
* `top_k_categorical_accuracy`
* `sparse_top_k_categorical_accuracy`
* `cosine_proximity`
* any loss function
]

.pull-right[
&lt;img src="img/neural_network_metrics.png" width="400" height="400" style="display: block; margin: auto;" /&gt;
]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
As discussed, any .KULbginline[loss function] can be also used as an accuracy .KULbginline[metric].


In the case of the MNIST dataset, we search for a model with a high .KULbginline[accuracy]. Hereto, we calculate the number of times the class of the observed `\(y\)` equals the predicted class `\(\hat{y}\)`, and divide by the size of the (training) set.

.hi-pink[Q]: Why can we not use .KULbginline[accuracy] as our loss function?
]

---

# Fitting the model

`fit(.)` tunes the model parameters (the weights and bias terms). 

We use `fit()` to start executing model training.

.pull-left[


```r
model %&gt;%
* fit(input,
*     output,
      batch_size = 128, 
      epochs = 10, 
      validation_split = 0.2)
```

]

.pull-right[
The first arguments are the input data (here: training images stored in `input`) and their corresponding class (here: 0-9, the labels of the training data, stored in `output`).
]


---
# Fitting the model (cont.)

`fit(.)` tunes the model parameters (the weights and bias terms). 

We use `fit()` to start executing model training.

.pull-left[


```r
model %&gt;%
  fit(input, 
      output, 
*     batch_size = 128,
*     epochs = 10,
      validation_split = 0.2)
```

Parameter updates are calculated based on small subsets of the training data with `batch_size` elements. 

An `epoch` is one iteration of the algorithm over the full dataset. 

]

.pull-right[
  &lt;img src="img/neural_network_epoch.gif" width="400" height="300" style="display: block; margin: auto;" /&gt;
.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]
]

---

# Three variants of gradient descent

.pull-left[

With .KULbginline[batch] gradient descent: 

* compute loss for each observation in the training data
* update parameters after all training examples have been evaluated
* .hi-pink[con]: scales horribly to bigger data sets.

With .KULbginline[stochastic] gradient descent: 

* randomly select an observation, compute gradient
* update parameters after this single observation has been evaluated
* .hi-pink[con]: takes a long time to convergence.
]

.pull-right[

With .KULbginline[mini-batch] gradient descent: 

* randomly select a subset of the training observations, compute gradient
* update parameters after this subset has been evaluated.

.KULbginline[Pros]: 

* balance efficiency of batch vs stochastic
* balance robust convergence of batch with some stochastic nature to avoid local minima.

.hi-pink[Cons]: 

* additional tuning parameter. 

]

---

# Fitting the model (cont.)

`fit(.)` tunes the model parameters (the weights and bias terms). 

.pull-left[


```r
model %&gt;%
  fit(input, 
      output, 
      batch_size = 128, 
      epochs = 10,
*     validation_split = 0.2)
```



]

.pull-right[

With the `validation_split = 0.2` we use the last 20% of our input training data as a hold-out validation set.

We evaluate the loss on this validation set at the end of each epoch.

]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
&lt;br&gt;
You will now .KULbginline[design], .KULbginline[compile] and .KULbginline[fit] your own neural network for the MNIST dataset. 


As a form of paralellized model selection, all of us will play with different model parameters. This way we gain insight into which parameter values work well for this dataset.
&lt;br&gt;
&lt;br&gt;
**Base model**: the neural network with a single hidden layer, as specified in the R script.
&lt;br&gt;
&lt;br&gt;
Try some of the following ideas to improve the model: (more ideas on the next slide!)

* .KULbginline[add hidden layers]: the number of nodes in subsequent layers should decrease

* .KULbginline[change batch size]

* .KULbginline[change the activation function].
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

Examples of .KULbginline[layer types]

* [`layer_gaussian_noise`](https://keras.io/layers/noise/): adds gaussian noise N(0, stddev) to the nodes when training the model. This reduces the probability of overfitting.

```r
model &lt;- model %&gt;%
  layer_gaussian_noise(stddev)
```

* [`layer_dropout`](https://keras.io/layers/core/): sets a fraction `rate` of the input units to zero. This reduces the probability of overfitting.

```r
model &lt;- model %&gt;%
  layer_dropout(rate)
```

* [`layer_batch_normalization`](https://keras.io/layers/normalization/): centers and scales the values of each node in the previous layer.

```r
model &lt;- model %&gt;%
  layer_batch_normalization()
```


]

---

class: clear

.pull-left[

Several methods proposed to reduce overfitting:

* try different weight initializations
* early stopping
* regularization
* dropout.

Available in {keras} as: 


```r
layer_dense(units = , activation = "relu", 
      kernel_regularizer = regularizer_l2(l = 0.001))
```


```r
layer_dropout(0.6)
```


```r
# fit with callbacks
model %&gt;% fit(___, ___, callbacks = list(
  callback_early_stopping(___)
))
```

Fore more details, please consult [https://keras.rstudio.com/articles/training_callbacks.html](https://keras.rstudio.com/articles/training_callbacks.html).

]

.pull-right[

With .KULbginline[early stopping]:
* calculate validation performance after each epoch
* stop when this no longer improves.

With .KULbginline[regularization] *(cfr. lasso and {glmnet})*: e.g.
`$$\min_{w,b} \mathcal{L}(w,b)+\frac{\lambda}{2}\cdot \|w\|_2^2.$$`

With .KULbginline[dropout]:
* randomly set activations to zero, with fixed probability `\(p\)`
* both in forward propagation as well as backpropagation
* only in training, all nodes turned on during prediction.

]

---
# Model evaluation

`evaluate(.)` calculates losses and metrics on the test dataset. 

```r
model %&gt;% 
  evaluate(test_input, test_output, verbose = 0)
##      loss  accuracy 
## 0.2336413 0.9341000
```

`predict(.)` returns a vector of length 10 with the probability per output node.

```r
prediction &lt;- model %&gt;% 
  predict(test_input)

round(prediction[1, ], 3)
##  [1] 0.000 0.000 0.001 0.003 0.000 0.000 0.000 0.995 0.000 0.001
```

The predicted category is the node with the highest probability.

```r
category &lt;- apply(prediction, 1, which.max) - 1
actual_category &lt;- apply(test_output, 1, which.max) - 1
```


---


# Model evaluation (cont.)

We inspect the misclassified images to gain more insight in the model.

Below we show some examples for our pre-trained MNIST neural network.

.pull-left[

```r
head(which(actual_category != category))
## [1]  9 34 39 64 88 93
```


```r
index &lt;- 9
plot_image(test_input[index, ]) +
  ggtitle(paste(
    'actual: ', actual_category[index], 
    ' predicted: ', category[index], sep='')) +
  theme(legend.position = 'none', 
        plot.title = element_text(hjust = 0.5))
```
]

.pull-right[

&lt;img src="img/neural_network_misclassification.png" width="700" height="300" style="display: block; margin: auto;" /&gt;

]
---
# Model evaluation (cont.)

We inspect the images for which the model assigns the lowest probability to the correct class.


```r
# select per row, the probability corresponding to the correct class
prob_correct &lt;- prediction[cbind(1:nrow(prediction), actual_category+1)]

# get the index of the 5 lowest records in prob_correct
which(rank(prob_correct) &lt;= 5)
## [1] 1501 5735 6167 6506 6652
```

&lt;img src="img/neural_network_worst.png" width="700" height="300" style="display: block; margin: auto;" /&gt;
---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
&lt;br&gt;
You will now .KULbginline[evaluate] your own model!
&lt;br&gt;&lt;br&gt;
* .hi-pink[Q.1]: calculate the accuracy of your model on the test set. &lt;br&gt; &lt;br&gt;
* .hi-pink[Q.2]: visualize some of the misclassified images from your model. &lt;br&gt;&lt;br&gt;
* .hi-pink[Q.3]: generate an image consisting of random noise and let the model classify this image. What do you think of the results? &lt;br&gt;
Remember: your input should be a 1x784 matrix with values in [0, 1].
]

---
# Feeding random data to a neural network

.pull-left[

```r
random &lt;- matrix(runif(28^2), nrow = 1)
plot_image(random[1, ])
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-80-1.png" width="360" height="360" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
round(predict(model, random), 3)
##      [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
## [1,]    0    0 0.617 0.124 0.002 0.196 0.042 0.001 0.018     0
```

Our pre-trained MNIST model is pretty sure that the input on the left is a two!
]

---
# Model understanding

Inspecting the calibrated weights can provide some insight in the features created in the hidden layer.

.pull-left[
Every node in the first hidden layer has 784 connections with the input layer. 

The weights of these connections can be visualized as an 28x28 image.


```r
node &lt;- 9
layer &lt;- 1
weights &lt;- model$get_weights()[[2*(layer-1) + 1]][, node]

plot_image(as.numeric(weights))
```

On the right we show a visualization of the calibrated weights for a pre-trained model with (only) 16 nodes in the first hidden layer.
]

.pull-right[

&lt;img src="img/neural_network_calibrated_weights.png" width="400" height="400" style="display: block; margin: auto;" /&gt;
]
---
# Summary of the fundamentals

.pull-left[

We discussed so far: 

* design neural networks .KULbginline[sequentially] in {keras}
`keras_model_sequential`. 

* layers consist of .KULbginline[nodes] and .KULbginline[connections].

* vanilla choice is a .KULbginline[fully connected layer]
`layer_dense`

* .KULbginline[fit] the model via gradient descent (i.e. backpropagation).
]

.pull-right[

List of .KULbginline[tuning/architectural] choices:

  - the number of layers
  - the number of nodes per layer
  - the activation functions
  - the layer type *(more on this coming soon)*
  - the loss function
  - the optimization algorithm
  - the batch size
  - the number of epochs
  - ...
]

---

class: clear

&lt;div style='height:100%; overflow:scroll;'&gt;
&lt;img src = "img/neural_network_types.png" /&gt;
&lt;/div&gt;

---

class: inverse, center, middle
name: regression

# Claim frequency and severity regression

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---
# Preparing the MTPL data
We predict .KULbginline[claim frequency] and .KULbginline[severity] in the MTPL data with a .KULbginline[neural network].

Load the .hi-pink[MTPL] data:

```r
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) 
mtpl &lt;- read.table('../data/PC_data.txt',
                   header = TRUE, stringsAsFactors = TRUE) %&gt;% 
  as_tibble() %&gt;% rename_all(tolower) %&gt;% rename(expo = exp)
```

Create a .hi-pink[training] and .hi-pink[test] set with {rsample}:

```r
library(rsample)
set.seed(54321)
data_split &lt;- initial_split(mtpl)
mtpl_train &lt;- training(data_split)
mtpl_test  &lt;- testing(data_split)
# Reshuffling of the training observations
mtpl_train &lt;- mtpl_train[sample(nrow(mtpl_train)), ]
```

---
# Regression with neural networks

In Module 1 we fitted .KULbginline[GLMs] for .hui-pink[claim frequency] as follows:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{x}^{'}\color{#20B2AA}{\beta})).$$
&lt;br&gt;

We now .hi-pink[redefine] this model as a .KULbginline[neural network]:

.center[
Formula |  GLM  | Neural network
------------- | -------------
`\(\color{#FFA500}{Y}\)`   | response | output node
&amp;nbsp; Poisson &amp;nbsp; | &amp;nbsp; distribution &amp;nbsp; | loss function
exp            | inverse link function | &amp;nbsp; activation function &amp;nbsp;
`\(\color{#e64173}{x}\)`  | predictors | input nodes
`\(\color{#20B2AA}{\beta}\)` | fitted effect | weights
]

---
# Your first claim frequency NN

Let's start with a model with .KULbginline[only an intercept]:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{1} \cdot \color{#20B2AA}{\beta})).$$

.pull-left[

```r
nn_freq_intercept &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

.hi-pink[Q.]: How many parameters does this model have?
]

.pull-right[
* `layer_dense`: there are .hi-pink[no hidden layers], the input layer is directly connected to the output layer.

* `units = 1`: there is .hi-pink[one] output node.

* `activation = 'exponential'`: we use an .hi-pink[exponential] inverse link function.

* `input_shape = c(1)`: there is .hi-pink[one] input node, i.e., the intercept which will be constant one.

* `use_bias = FALSE`: we don't need a .hi-pink[bias] term, since we explicitly include a constant intercept. 

* `loss = 'poisson'`: we maximize the .hi-pink[Poisson] likelihood.
]

---
# Your first claim frequency NN

Let's start with a model with .KULbginline[only an intercept]:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{1} \cdot \color{#20B2AA}{\beta})).$$

.pull-left[

```r
nn_freq_intercept &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

.hi-pink[Q.]: How many parameters does this model have?

```r
nn_freq_intercept$count_params()
## [1] 1
```

]

.pull-right[
* `layer_dense`: there are .hi-pink[no hidden layers], the input layer is directly connected to the output layer.

* `units = 1`: there is .hi-pink[one] output node.

* `activation = 'exponential'`: we use an .hi-pink[exponential] inverse link function.

* `input_shape = c(1)`: there is .hi-pink[one] input node, i.e., the intercept which will be constant one.

* `use_bias = FALSE`: we don't need a .hi-pink[bias] term, since we explicitly include a constant intercept. 

* `loss = 'poisson'`: we maximize the .hi-pink[Poisson] likelihood.
]

---
# Your first claim frequency NN (cont.)

.pull-left[
Create .hi-pink[vectors] for the input and output:


```r
intercept &lt;- rep(1, nrow(mtpl_train))

counts &lt;- mtpl_train$nclaims
```

.KULbginline[Fit] the neural network:

```r
nn_freq_intercept %&gt;% fit(x = intercept,
                          y = counts,
                          epochs = 30,
                          batch_size = 1024, 
                          validation_split = 0,
                          verbose = 0)
```



]

.pull-right[
* `x = intercept`: use the intercept as .hi-pink[feature].

* `y = counts`: use the claim counts as .hi-pink[target].

* `epochs = 20`: perform 20 training .hi-pink[iterations] over the complete data.

* `batch_size = 1024`: use .hi-pink[batches] with 1024 observations to update weights.

* `validation_split = 0`: don't use a .hi-pink[validation] set, so all observations are used for training. 

* `verbose = 0`: .hi-pink[silence] keras such that no output is generated during fitting.
]

---
# Comparing our neural network with a GLM

We .KULbginline[compare] the results of our neural network with the same model specified as a GLM:


```r
glm_freq_intercept &lt;- glm(nclaims ~ 1,
                          data = mtpl_train,
                          family = poisson(link = 'log'))

# GLM coefficients
glm_freq_intercept$coefficients
## (Intercept) 
##   -2.084486

## NN weights
nn_freq_intercept$get_weights()
## [[1]]
##           [,1]
## [1,] -2.085138
```

There is a small difference in the parameter estimate, resulting from a .hi-pink[different optimization technique].


---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

We have shown that a .KULbginline[Poisson GLM] can be implemented as a neural network.
&lt;br&gt;

* .hi-pink[Q1]: adapt this code to replicate a .KULbginline[binomial GLM] with a .hi-pink[logit] link function. Add .KULbginline[accuracy] as a .hi-pink[metric] in your model.&lt;br&gt;
Hint 1: the `sigmoid` activation function is the inverse of the logit link function. &lt;br&gt;
Hint 2: the `binary_crossentropy` loss maximizes the loglikelihood of Bernoulli outcomes:
$$ \sum_{i=1}^n (\color{#FFA500}{y_i} \cdot \log(p_i) + (1-\color{#FFA500}{y_i}) \cdot \log(1-p_i)).$$

* .hi-pink[Q2]: .KULbginline[fit] your NN on the outcome variable (nclaims &gt; 0), i.e. modeling no claim versus having at least one claim. 

* .hi-pink[Q3]: .KULbginline[compare] your fitted neural network with a .hi-pink[GLM].
]

---
class:clear

.pull-left[
.hi-pink[Q1]: set-up for .KULbginline[binary classification]

```r
nn_binary &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'sigmoid', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```

.hi-pink[Q2]: .KULbginline[fit] your NN

```r
nn_binary %&gt;% fit(x = intercept,
                  y = counts &gt; 0,
                  epochs = 40,
                  batch_size = 1024, 
                  validation_split = 0,
                  verbose = 0)
```


]

.pull-right[
.hi-pink[Q3]: compare with a .KULbginline[GLM]

```r
glm_binary &lt;- glm((nclaims &gt; 0) ~ 1, 
                  data = mtpl_train, 
                  family = binomial(link = 'logit'))

glm_binary$coefficients
## (Intercept) 
##   -2.064613
nn_binary$get_weights()[[1]] %&gt;% as.numeric()
## [1] -2.063514

unique(predict(glm_binary, type = 'response'))
## [1] 0.1125841
unique(predict(nn_binary, x = intercept))
##          [,1]
## [1,] 0.112694
```

]
---
# Taking exposure into account in a NN

.pull-left[
The .KULbginline[Poisson loss] function, including .hi-pink[exposure], is

$$ \mathcal{L} = \sum_i \texttt{expo}_i \cdot \lambda_i - Y_i \cdot \log(\texttt{expo}_i \cdot \lambda_i),$$
which can be rewritten as follows:

$$ \mathcal{L} = \sum_i \texttt{expo}_i \cdot (\lambda_i - \frac{Y_i}{\texttt{expo}_i} \log(\lambda_i)).$$
This is the loss function for a Poisson model with:

* observations `\(\frac{Y_i}{\texttt{expo}_i}\)` and
* weights `\(\texttt{expo}_i\)`.
]

.pull-right[
Notice indeed how the parameter estimates of the following two GLMs are .hi-pink[identical]:

```r
glm_offset &lt;- glm(nclaims ~ ageph,
                  family = poisson(link = 'log'),
                  data = mtpl_train,
                  offset = log(expo))
glm_offset$coefficients
## (Intercept)       ageph 
## -1.24456257 -0.01582612

glm_weights &lt;- glm(nclaims / expo ~ ageph,
                   family = poisson(link = 'log'),
                   data = mtpl_train,
                   weights = expo)
glm_weights$coefficients
## (Intercept)       ageph 
## -1.24456257 -0.01582612
```
]

---
# Taking exposure into account in a NN (cont.)

.pull-left[
.KULbginline[Nothing] changes in our NN model .hi-pink[specification]:

```r
nn_freq_exposure &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

It is however .hi-pink[good practice] to always .KULbginline[recompile]. &lt;br&gt; 
Otherwise the NN will pick up where it left off last time, with the optimal weights after fitting.

]

.pull-right[
Create a vector with exposure values:

```r
exposure &lt;- mtpl_train$expo
```

Divide claim counts by exposure and use weights:

```r
nn_freq_exposure %&gt;% 
  fit(x = intercept,
*     y = counts / exposure,
*     sample_weight = exposure,
      epochs = 20,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```

.KULbginline[Stay tuned] to find out how to include exposure via an .hi-pink[offset] term!

]

---
# Adding an input feature and a hidden layer

.pull-left[
Let's start by adding .hi-pink[one feature], namely `ageph`:

```r
ageph &lt;- mtpl_train$ageph
```

Define the NN .hi-pink[architecture]:

```r
nn_freq_ageph &lt;- 
  keras_model_sequential() %&gt;%
  layer_batch_normalization(input_shape = c(1)) %&gt;%
  layer_dense(units = 5,
              activation = 'tanh') %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              use_bias = TRUE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```
]

.pull-right[
* .KULbginline[Pre-processing] (see Module 1): 

`layer_batch_normalization` .hi-pink[centers] and .hi-pink[scales] the input features (here only one) .hi-pink[per batch].

* .KULbginline[Hidden] layer:

`layer_dense` with five nodes and the `tanh` activation function.

* .KULbginline[Output] layer:

`layer_dense` with one node and the `exponential` activation function.&lt;br&gt;
Notice how we set `use_bias = TRUE` for the .hi-pink[intercept.]

]

---
# Adding an input feature and a hidden layer (cont.)

.pull-left[
Let's .hi-pink[fit] our brand new NN:

```r
nn_freq_ageph %&gt;% 
* fit(x = ageph,
      y = counts / exposure,
      sample_weight = exposure,
      epochs = 30,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```



We also fit a .hi-pink[GAM] to `ageph`:

```r
library(mgcv)
gam_ageph &lt;- gam(nclaims ~ s(ageph),
                 data = mtpl_train, 
                 family = poisson(link = 'log'), 
                 offset = log(expo))
```

.hi-pink[Q.]: What do you think about those fits?
]

.pull-right[
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-106-1.png" style="display: block; margin: auto;" /&gt;


]
---
# Adding a skip connection in a NN

So far, we stayed in a .hi-pink[purely sequential] architecture with `keras_model_sequential()`.

Now, we will allow some input nodes to be connected directly to the output node, i.e., .KULbginline[skip connections].

.pull-left[
&lt;img src="img/CANNarchitecture.png" width="75%" style="display: block; margin: auto;" /&gt;
Figure taken from [Schelldorfer and Wuthrich (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525)
]

.pull-right[
The output node, without skip connection, calculates:
`$$f_{activation}(\sum_i w_i h_i + b).$$`

With a skip connection, this simply becomes:
`$$f_{activation}(\sum_i w_i h_i + b + s).$$`
We take a .hi-pink[linear] combination of the last hidden layer outputs and .hi-pink[add] the skip input, .hi-pink[before] applying the activation function.

So, what can we do with this?
]

---
# Adding a skip connection in a NN (cont.)

Let's take a .KULbginline[claim frequency] example with the `exponential` activation function.

* Adding exposure as an .hi-pink[offset] term:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(expo)}) = \color{#e64173}{expo} \cdot \exp(\sum_i w_i h_i + b).$$`

* Adding a .hi-pink[base] prediction:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(base)}) = \color{#e64173}{base} \cdot \exp(\sum_i w_i h_i + b).$$`

* The .hi-pink[combination] of both:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(expo \cdot base)}) = \color{#e64173}{expo \cdot base} \cdot \exp(\sum_i w_i h_i + b).$$`

A skip connection allows us to guide the NN in the right direction and model .hi.pink[adjustments] to the base predictions, for example obtained via a GLM or GAM. In the actuarial lingo this is called a .KULbginline[C]ombined .KULbginline[A]ctuarial .KULbginline[N]eural .KULbginline[N]etwork (.hi-pink[CANN]).

---
# Adding a skip connection in a NN (cont.)

Create two .hi-pink[input layers] via `layer_input`, one for the skip connection and one for the NN:

```r
input_skip &lt;- layer_input(shape = c(1), name = 'skip')
input_nn &lt;- layer_input(shape = c(1), name = 'nn')
```

Specify the .hi-pink[architecture] to use for the NN part with a .hi-pink[linear] combination of the hidden nodes outputs:

```r
network &lt;- input_nn %&gt;% layer_batch_normalization() %&gt;% layer_dense(units = 5, activation = 'tanh') %&gt;%
  layer_dense(units = 1, activation = `'linear'`)
```

.hi-pink[Combine] the NN and skip connection via `layer_add` and pass through the `exponential` function without weights:

```r
output &lt;- list(network, input_skip) %&gt;% layer_add() %&gt;% 
  layer_dense(units = 1, activation = 'exponential', `trainable = FALSE`, name = 'output',
              weights = list(array(`1`, dim = c(1,1)), array(`0`, dim = c(1))))
```

Define the full model with .hi-pink[inputs] and .hi-pink[output] via `keras_model` and .hi-pink[compile] as usual:

```r
cann &lt;- keras_model(inputs = list(input_nn, input_skip), outputs = output)
cann %&gt;% compile(loss = 'poisson', optimize = optimizer_rmsprop())
```

---
# Adding a skip connection in a NN (cont.)

.pull-left[

<div id="htmlwidget-76c7b0e4dfca990a73d9" style="width:504px;height:504px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-76c7b0e4dfca990a73d9">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n       outputorder = \"edgesfirst\",\n       bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n      fontsize = \"10\",\n      shape = \"circle\",\n      fixedsize = \"true\",\n      width = \"0.5\",\n      style = \"filled\",\n      fillcolor = \"aliceblue\",\n      color = \"gray70\",\n      fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n     fontsize = \"8\",\n     len = \"1.5\",\n     color = \"gray80\",\n     arrowsize = \"0.5\"]\n\n  \"1\" [label = \"nn\nInputLayer\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,6!\"] \n  \"2\" [label = \"batch_normalization_1\nBatchNormalization\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,5!\"] \n  \"3\" [label = \"dense_8\nDense\ntanh\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,4!\"] \n  \"4\" [label = \"dense_7\nDense\nlinear\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,3!\"] \n  \"5\" [label = \"skip\nInputLayer\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"1.5,6!\"] \n  \"6\" [label = \"add\nAdd\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.75,2!\"] \n  \"7\" [label = \"output\nDense\nexponential\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0.75,1!\"] \n  \"1\"->\"2\" \n  \"2\"->\"3\" \n  \"3\"->\"4\" \n  \"4\"->\"6\" \n  \"5\"->\"6\" \n  \"6\"->\"7\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
]

.pull-right[
Calculate the GAM .hi-pink[base] predictions, including .hi-pink[exposure]:

```r
gam_expo &lt;- predict(gam_ageph) + log(mtpl_train$expo)
```

Collect the CANN input data in a .hi-pink[named list]:

```r
cann_input &lt;- list('nn' = mtpl_train$ageph,
                   'skip' = gam_expo)
```

.hi-pink[Fit] the CANN like we have seen before:

```r
cann %&gt;% fit(x = cann_input,
             y = counts,
             epochs = 20,
             batch_size = 1024, 
             validation_split = 0,
             verbose = 0)
```
]



---
# Adding a skip connection in a NN (cont.)

.pull-left[
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-117-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-118-1.png" style="display: block; margin: auto;" /&gt;
]

---
# Claim severity modeling with NNs

Within the GLM framework, .KULbginline[claim severity] is often modeled via a .hi-pink[lognormal] or .hi-pink[gamma] distribution.

.pull-left[
Let's model the .hi-pink[log severity] with a .hi-pink[MSE] loss:

```r
nn_sev_log &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 1, activation = 'linear', 
              input_shape = c(1), use_bias = FALSE) %&gt;%
* compile(loss = 'mse',
          optimize = optimizer_rmsprop())

claims &lt;- mtpl_train %&gt;% dplyr::filter(nclaims &gt; 0)

nn_sev_log %&gt;% 
  fit(x = rep(1, nrow(claims)),
*     y = log(claims$avg),
      epochs = 100, batch_size = 128, 
      validation_split = 0, verbose = 0)

predict(nn_sev_log, 1) %&gt;% exp() %&gt;% as.numeric()
## [1] 450.1818
claims$avg %&gt;% summary()
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
##       0.0     142.2     523.5    1605.8    1426.4 1989567.9
```
]

.pull-right[
Seems like we are heavily .KULbginline[underestimating] the mean?

The mean and median of the .hi-pink[lognormal] distribution:
`$$mean = \exp(\mu + \sigma^2/2)$$`
`$$median = \exp(\mu)$$`
We are doing the latter, but how to .hi-pink[estimate] `\(\sigma\)` in a NN?

&lt;br&gt;

.KULbginline[Alternatives] to consider:

* use MSE with .hi-pink[untransformed] severity amounts,

* implement .hi-pink[your own loss] function (e.g., gamma).

]
---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

Time to .KULbginline[apply] everything you learned on neural networks in a .hi-pink[claim frequency case study]! &lt;br&gt;
The goal is to .hi-pink[fit] a couple of NNs on `mtpl_train` and .hi-pink[evaluate] those models on `mtpl_test`.

**Step 1**: .KULbginline[Data pre-processing]:

* Write a .hi-pink[recipe] to prepare the input data.  &lt;br&gt; Feel free to check the {recipes} [reference page](https://tidymodels.github.io/recipes/reference/index.html) for inspiration.

* Your recipe should contain at least the following .hi-pink[steps]:&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; Remove the features `id`, `amount`, `avg`, `town` and `pc` with `step_rm`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; Normalize all numeric features with `step_normalize`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; Dummy encode all nominal features with `step_dummy`.&lt;br&gt;

* .hi-pink[Bake] the training and test data following your recipe.

* Make the train and test data .hi-pink[NN proof] as follows:&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; Create .hi-pink[vectors] for both `nclaims` and `expo`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; Create a .hi-pink[matrix] for the input features via `as.matrix()`.
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

**Step 2**: .KULbginline[Model building]:

* .hi-pink[Compile] and .hi-pink[fit] a couple of neural networks on the pre-processed training data. &lt;br&gt; You are completely free to specify the architecture of your choice. 

* You can .hi-pink[experiment] with the following options:&lt;br&gt;
&amp;#9675; the number of layers,&lt;br&gt;
&amp;#9675; the number of nodes per layer,&lt;br&gt;
&amp;#9675; the batch size,&lt;br&gt;
&amp;#9675; the activation functions,&lt;br&gt;
&amp;#9675; adding dropout layers,&lt;br&gt;
&amp;#9675; including batch normalization after some layers,&lt;br&gt;
&amp;#9675; ...&lt;br&gt;

* If you want to include exposure, either use .hi-pink[weights] or a .hi-pink[skip] connection.

**Step 3.** .KULbginline[Model evaluation]:

* .hi-pink[Evaluate] your models on the test data via `evaluate`. Which architecture is the winner?

* .hi-pink[Compare] your favorite neural network with a GLM, GAM or GBM. Which model is preferred?

]

---

class:clear

.pull-left[
.hi-pink[Step 1]: data pre-processing with {recipes}

```r
library(recipes)

# Create and prepare the recipe
mtpl_recipe &lt;- recipe(nclaims ~ ., data = mtpl_train) %&gt;%
  step_rm(id, amount, avg, town, pc) %&gt;%
  step_nzv(all_predictors(), -expo) %&gt;%
  step_normalize(all_numeric(), -c(nclaims, expo)) %&gt;%
  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%
  prep(mtpl_train)

# Bake the training and test data
mtpl_train_b &lt;- mtpl_recipe %&gt;% juice()
mtpl_test_b &lt;- mtpl_recipe %&gt;% bake(new_data = mtpl_test)

# Make the data NN proof
train_x &lt;- mtpl_train_b %&gt;% 
  dplyr::select(-c(nclaims, expo)) %&gt;% as.matrix()
test_x &lt;- mtpl_test_b %&gt;% 
  dplyr::select(-c(nclaims, expo)) %&gt;% as.matrix()

train_y &lt;- mtpl_train_b$nclaims
test_y &lt;- mtpl_test_b$nclaims

train_expo &lt;- mtpl_train_b$expo
test_expo &lt;- mtpl_test_b$expo
```
]

.pull-right[
.hi-pink[Step 2]: a possible NN architecture with 2 hidden layers

```r
nn_case &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 20,
              activation = 'relu', 
              input_shape = ncol(train_x)) %&gt;%
  layer_dense(units = 10,
              activation = 'relu') %&gt;% 
  layer_dense(units = 1, 
              activation = 'exponential') %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_nadam())
  
nn_case %&gt;%
  fit(x = train_x,
      y = train_y / train_expo,
      sample_weight = train_expo,
      epochs = 20,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```


]

---

class:clear

.pull-left[
.hi-pink[Step 3a]: evaluate the NN on the test data

```r
# Built-in evaluation
nn_case %&gt;%
  evaluate(x = test_x,
           y = test_y,
           verbose = 0)
##      loss 
## 0.3741113

# If you want to check the results
poisson_loss &lt;- function(pred, actual) {
  mean(pred - actual * log(pred))
}
poisson_loss(predict(nn_case, test_x),
             test_y)
## [1] 0.3741113

# Use array for weights in evaluate
nn_case %&gt;% 
  evaluate(x = test_x,
           y = test_y,
           sample_weight = array(test_expo),
           verbose = 0)
##      loss 
## 0.3413752
```

]

.pull-right[
.hi-pink[Step 3b]: compare the NN performance with a GAM

```r
gam_case &lt;- gam(
  nclaims ~ coverage + fuel + sex +
    s(ageph) + s(bm) + s(power) + s(agec),
  data = mtpl_train,
  offset = log(expo),
  family = poisson(link = 'log')
  )

poisson_loss(predict(gam_case, mtpl_test,
                     type = 'response'),
             test_y)
## [1] 0.3746941
```


]

---

class: inverse, center, middle
name: autoencoder

# Auto encoders

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---
# Auto encoders

.KULbginline[Auto encoders] compress the input data into a limited number of features. 

.pull-left[
&lt;img src="img/neural_network_auto_encoder.png" width="300" height="400" style="display: block; margin: auto;" /&gt;
]

.pull-right[

* .KULbginline[Unsupervised] machine learning algorithm. 

* .KULbginline[Decorrelation] of the input data, comparable with PCA. The low dimensional compressed data is often used as an input in traditional statistical models.

* Input and output are identical.

* Few nodes in the center of the network. This is the compressed feature space.

* A high performing auto encoder is capable of reconstructing the input data based on compressed feature space.
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

Auto encoders can be implemented in Keras using the same tools that you have already learned during this course. 
&lt;br&gt;
The following steps guide you in .KULbginline[constructing] and .KULbginline[training] your personal auto encoder for the MNIST dataset.

* Make a sketch of the neural network that you will implement.

* Define a neural network with 5 layers:
  * Layer 1: input (784 nodes)
  * Layer 2: Hidden layer (128 nodes)
  * Layer 3: Hidden layer (32 nodes), this is the compressed feature space
  * Layer 4: Hidden layer (128 nodes)
  * Layer 5: Output layer (784 nodes)

* Choose an appropriate activation function for each layer:
  * identity
  * ReLU
  * sigmoid
  * softmax
]
---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

* Which of these loss functions can we use to train the model?
  * mse
  * binary_crossentropy
  * categorical_crossentropy

* Fit the model on the MNIST data in 10 epochs.

* Experiment with adding other layer types to the model:
  * layer_gaussian_noise(stddev)
  * layer_dropout(rate)
  * layer_batch_normalization()
]

---
class: clear

.pull-left[

```r
*encoder &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = 'sigmoid')

*model &lt;- encoder %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid') %&gt;%
  layer_dense(units = 784, activation = 'sigmoid') %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
  fit(input, 
      input, 
      epochs = 10, 
      batch_size = 256, 
      shuffle = TRUE, 
      validation_split = 0.2)
```
]

.pull-right[
`encoder` contains the first part of the model for compressing the model.

`model` is the full auto encoder, including the encode and decode step.

By defining `model` as an extension of `encoder`, we can compress the data using `predict(encoder, ...)` after training the model.
]

---
class: clear

.pull-left[

```r
encoder &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 128, activation = `'sigmoid'`, 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = `'sigmoid'`)

model &lt;- encoder %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = `'sigmoid'`) %&gt;%
  layer_dense(units = 784, activation = `'sigmoid'`) %&gt;%
  compile(loss = `'binary_crossentropy'`,
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
  fit(input, 
      input, 
      epochs = 10, 
      batch_size = 256, 
      shuffle = TRUE, 
      validation_split = 0.2)
```
]

.pull-right[
I interpret the hidden nodes as binary features and therefore use a `sigmoid` activation function. 

We no longer use the `softmax` activation function in the last layer, since multiple output nodes can be activated simultaneously.

I choose `binary_crossentropy` as a loss function, since we have independent bernoulli outcome variables.

Another good combination would have been:
* activation `ReLU` in the hidden layers
* activation `identity` in the output layer
* `mse` as the loss function
]

---
class:clear

.pull-left[

```r
encoder &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 128, activation = 'sigmoid', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = 'sigmoid')

model &lt;- encoder %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid') %&gt;%
  layer_dense(units = 784, activation = 'sigmoid') %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
* fit(input,
*     input,
      epochs = 10, 
      batch_size = 256, 
*     shuffle = TRUE,
      validation_split = 0.2)
```
]

.pull-right[
The `input` variable is also passed to the model as the `output` parameter.

The option `shuffle=TRUE` shuffles the training dataset after each epoch, such that the model is trained on different batches.

]

---
# The big test

Let's compare the input and output of our auto encoder.


```r
result &lt;- predict(model, input[1, , drop = FALSE])
plot_image(input[1, ]) # the original image
plot_image(result[1, ]) # the reconstruction of the model
```


&lt;img src="img/neural_network_auto_encoder_comparison.png" width="750" height="370" style="display: block; margin: auto;" /&gt;


---
# What happens with random noise?


```r
random &lt;- matrix(runif(28^2), nrow = 1)

gridExtra::grid.arrange(
  plot_image(random[1, ]) + theme(legend.position = 'none'),
  plot_image(predict(model, random)[1, ]) + theme(legend.position = 'none'),
  nrow = 1)
```

&lt;img src="img/neural_network_auto_encoder_noise.png" width="500" height="300" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle
name: cnn

# Convolutional neural networks (CNN)

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---
# Convolutional neural networks (CNN)

.pull-left[
So far, the first step in our analysis was to flatten the image matrix into a vector.


```r
input &lt;- tensorflow::array_reshape(
  input, c(nrow(input), 28*28)) / 255
```


This approach
* is not translation invariant. A completely different set of nodes gets activated when the image is shifted.

* ignores the dependency between nearby pixels.

* requires a large number of parameters/weights as each node in the first hidden layer is connected to all nodes in the input layer.

.KULbginline[Convolutional layers] allow to handle dimensional data, .hi-pink[without] flattening.

]

.pull-right[
&lt;img src="img/neural_network_flattening.png" width="300" height="350" style="display: block; margin: auto;" /&gt;

.right[Source: [Sumit Saha](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)]
]

---
# Convolutional layers

Classical hidden layers use .hi-pink[1 dimensional inputs] to construct .hi-pink[1 dimensional features].

2d convolutional layers use .hi-pink[2 dimensional input] (images) to construct .hi-pink[2 dimensional feature maps].

.pull-left[
The weights in a 2d convolutional layer are structured in a small image, called the kernel or the filter.

&lt;img src="img/neural_network_convolution_op1.png" width="300" height="200" style="display: block; margin: auto;" /&gt;

]

.pull-right[
We slide the kernel over the input image and compute matrix multiplications between the selected part of the image and the kernel.

&lt;img src="img/neural_network_convolution_op2.png" width="400" height="250" style="display: block; margin: auto;" /&gt;

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]
]

---
# Convolutional layers (cont.)

Classical hidden layers use .hi-pink[1 dimensional inputs] to construct .hi-pink[1 dimensional features].

2d convolutional layers use .hi-pink[2 dimensional input] (images) to construct .hi-pink[2 dimensional feature maps].

.pull-left[
The weights in a 2d convolutional layer are structured in a small image, called the kernel or the filter.

&lt;img src="img/neural_network_convolution_op1.png" width="300" height="200" style="display: block; margin: auto;" /&gt;

]

.pull-right[
We slide the kernel over the input image and compute matrix multiplications between the selected part of the image and the kernel.

&lt;img src="img/neural_network_convolution_op3.gif" width="400" height="250" style="display: block; margin: auto;" /&gt;
.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]
]

---
# Convolutional layers (cont.)

2d convolutional layers can .hi-pink[detect] the same, .hi-pink[local feature anywhere] in the image.

.pull-left[
A useful feature for classifying the number four is the presence of straight, vertical lines.

.hi-pink[Q]: How should the kernel look to detect this feature?

]

.pull-right[
&lt;img src="img/neural_network_mnist4.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
]

---
# Convolutional layers (cont.)

2d convolutional layers can .hi-pink[detect] the same, .hi-pink[local featur anywhere] in the image.

.pull-left[
A useful feature for classifying the number four is the presence of straight, .hi-pink[vertical lines].

.hi-pink[Q]: How should the .hi-pink[kernel] look to detect this feature?

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-140-1.png" width="200" height="200" style="display: block; margin: auto;" /&gt;


]

.pull-right[
original image:
&lt;img src="img/neural_network_mnist4.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
feature map:
&lt;img src="img/neural_network_feature_map.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
]
---
# Convolutional layers in {keras}

.pull-left[

```r
keras_model_sequential() %&gt;%
* layer_conv_2d(filters = 8,
*               kernel_size = c(3, 3),
*               strides = c(1, 1),
*               input_shape = c(28, 28, 1))
```

&lt;br&gt;

Add a .KULbginline[2d convolutional layer] with `layer_conv_2d()`:
]

.pull-right[
* `filters = 8`:

We construct .hi-pink[8 feature maps] associated to different kernels/.hi-pink[filters].

* `kernel_size = c(3, 3)`:

The filter/.hi-pink[kernel] has a size of .hi-pink[3x3].

* `strides = c(1, 1)`:

We .hi-pink[move] the .hi-pink[kernel] in steps of .hi-pink[1] pixel in both the horizontal and vertical direction. This is a common choice.

* `input_shape = c(28, 28, 1)`:

If this is the first layer of the model, we also have to specify the dimensions of the input data. The input consists of .hi-pink[1 image] of size .hi-pink[28x28].
]

---
# Pooling

A convolution layer is typically followed by a .hi-pink[pooling step], which reduces the size of feature maps.

.pull-left[
.hi-pink[Pooling layers] divide the image in blocks of equal size and then aggregate the data per block.

Two common operations are:

* Average pooling

```r
layer_average_pooling_2d(pool_size = c(2, 2),
                         strides = c(2, 2))
```

* Max pooling

```r
layer_max_pooling_2d(pool_size = c(2, 2),
                     strides = c(2, 2))
```


]

.pull-right[
&lt;img src="img/neural_network_pooling.jpg" width="380" height="240" style="display: block; margin: auto;" /&gt;

* `pool_size = c(2, 2)`: 

Pool blocks of 2x2

* `strides = c(2, 2)`:

Move in steps of size 2 in both the horizontal and vertical direction.

]

---
# Flatten

.pull-left[

```r
keras_model_sequential() %&gt;%
  layer_conv_2d() %&gt;%
  layer_max_pooling_2d() %&gt;%
* layer_flatten() %&gt;%
  layer_dense()
```
]

.pull-right[
Finally, when all local features are extracted the data is .KULbginline[flattened]. 

A classical neural network (as built in the first part of these course) analyzes these local features.
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

.KULbginline[Fit] and .KULbginline[evaluate] a .KULbginline[convolutional neural network] on the MNIST dataset.

Before you start, you should restructure the input and test dataset in the required format.


```r
# We start from the original data
input_conv &lt;- mnist$train$x / 255

# We add a fourth dimension, such that one data point has size (28x28x1)
input_conv &lt;- k_expand_dims(input_conv, axis = 4)
```

Once you are satisfied with the performance of your model, you can plot some images that were misclassified by your model.
]
---
class:clear

.pull-left[

```r
model &lt;- keras_model_sequential() %&gt;%
  layer_conv_2d(filters = 8, 
                kernel_size = 3, 
                input_shape = c(28, 28, 1)) %&gt;%
  layer_max_pooling_2d(pool_size = 2) %&gt;%
  layer_flatten() %&gt;%
  layer_dense(units = 10, 
              activation = 'softmax') %&gt;%
  compile(loss = 'categorical_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('accuracy')) 
  
model %&gt;% fit(input_conv, output, 
      epochs = 10, 
      batch_size = 128, 
      validation_split = 0.2)

model %&gt;% 
  evaluate(test_input_conv, 
           test_output,verbose = 0)
```
]

.pull-right[

```
##      loss  accuracy 
## 0.1224037 0.9659000
```

Model accuracy has increased significantly and improves further with more epochs. 

]
---
# Misclassifications


```r
prediction &lt;- model %&gt;% predict(test_input_conv)
category &lt;- apply(prediction, 1, which.max)-1
actual_category &lt;- apply(test_output, 1, which.max)-1
head(which(actual_category != category))
```

```
## [1]   9  19  93 248 260 319
```

```r
plot_image(test_input[93, ])
```


&lt;img src="img/neural_network_conv_wrong.png" width="700" height="300" style="display: block; margin: auto;" /&gt;

---
# What happens to random data?
.pull-left[

```r
random &lt;- runif(28*28)
random_conv &lt;- matrix(random, nrow = 28, ncol = 28)
random_conv &lt;- k_expand_dims(random_conv, axis = 1)
random_conv &lt;- k_expand_dims(random_conv, axis = 4)

plot_image(random)
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-156-1.png" width="300" height="300" style="display: block; margin: auto;" /&gt;
]

.pull-right[
There is not a single doubt, this has to be a nine!

```r
predict(model, random_conv)
##              [,1]         [,2]         [,3]         [,4]         [,5]
## [1,] 7.361005e-15 2.655454e-30 2.406724e-12 2.888002e-07 5.990133e-18
##              [,6]         [,7]         [,8]      [,9]        [,10]
## [1,] 3.955046e-15 6.562733e-10 2.638573e-21 0.9999998 2.079794e-08
```
Actually almost all random images will be classified as a nine.

]
---
# Inspecting the filter/kernel

The third filter/kernel is the one we infered for detecting vertical lines. 
Can you find an interpretation for the other kernels?


```r
weights &lt;- purrr::map(1:8, function(x) {plot_image(as.numeric(model$get_weights()[[1]][,,,x]), FALSE)})
weights[['nrow']] &lt;- 2
do.call(gridExtra::grid.arrange, weights)
```

&lt;img src="img/neural_network_conv_kernel.png" width="600" height="300" style="display: block; margin: auto;" /&gt;
---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

We have now built .KULbginline[convolutional neural] networks using .hi-pink[layer_conv_2d()]. &lt;br&gt;In addition {keras} defines  .hi-pink[layer_conv_1d()] and .hi-pink[layer_conv_3d()].
&lt;br&gt;
&lt;br&gt;
.hi-pink[Q]: For which data would you use .hi-pink[layer_conv_1d()]?
&lt;br&gt;

.hi-pink[Q]: Which function would you use to build a convolutional network for:

* Colored images
&lt;br&gt;

* Movie data
]

---

name: wrap-up

# Thanks!  &lt;img src="img/xaringan.png" class="title-hex"&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
&lt;br&gt; &lt;br&gt; &lt;br&gt;
Course material available via 
&lt;br&gt;
&lt;svg style="height:0.8em;top:.04em;position:relative;fill:#116E8A;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; https://github.com/katrienantonio/hands-on-machine-learning-R-module-3
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"highlightLines": true,
"countIncrementalSlides": false,
"highlightSpans": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
