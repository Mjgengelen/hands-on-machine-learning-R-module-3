<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Hands-on Machine Learning with R - Module 3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katrien Antonio, Jonas Crevecoeur &amp; Roel Henckaerts" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/metropolis.css" type="text/css" />
    <link rel="stylesheet" href="css/metropolis-fonts.css" type="text/css" />
    <link rel="stylesheet" href="css/my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">









class: bottom, left, title-slide, clear

background-image: url('img/beamer_title_KUL.png')
background-size: contain

name: titlepage

# Hands-on Machine Learning with R - Module 3

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=100%&gt;&lt;/html&gt;

&lt;span style="color:#FAFAFA;font-size:18.0pt"&gt;
Katrien Antonio &amp; Roel Henckaerts &amp; Jonas Crevecoeur &lt;br&gt; &lt;br&gt; 
&lt;/span&gt;

&lt;span style="color:#FAFAFA;font-size:14.0pt"&gt;
Webinar | October - November, 2023&lt;br&gt;
&lt;/span&gt;

---

class: inverse, center, middle
name: prologue

# Prologue

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---


name: introduction

# Introduction

### Course

<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> https://github.com/katrienantonio/hands-on-machine-learning-R-module-3

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

<svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c31.5 31.5 31.5 82.5 0 114L422.3 334.8c-31.5 31.5-82.5 31.5-114 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C206.5 251.2 213 330 263 380c56.5 56.5 148 56.5 204.5 0L579.8 267.7zM60.2 244.3c-56.5 56.5-56.5 148 0 204.5c50 50 128.8 56.5 186.3 15.4l1.6-1.1c14.4-10.3 17.7-30.3 7.4-44.6s-30.3-17.7-44.6-7.4l-1.6 1.1c-32.1 22.9-76 19.3-103.8-8.6C74 372 74 321 105.5 289.5L217.7 177.2c31.5-31.5 82.5-31.5 114 0c27.9 27.9 31.5 71.8 8.6 103.9l-1.1 1.6c-10.3 14.4-6.9 34.4 7.4 44.6s34.4 6.9 44.6-7.4l1.1-1.6C433.5 260.8 427 182 377 132c-56.5-56.5-148-56.5-204.5 0L60.2 244.3z"/></svg> [https://katrienantonio.github.io/](https://katrienantonio.github.io/) &amp; [https://be.linkedin.com/in/jonascrevecoeur](LinkedIn Jonas) &amp; [https://be.linkedin.com/in/roelhenckaerts](LinkedIn Roel)

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M16.1 260.2c-22.6 12.9-20.5 47.3 3.6 57.3L160 376V479.3c0 18.1 14.6 32.7 32.7 32.7c9.7 0 18.9-4.3 25.1-11.8l62-74.3 123.9 51.6c18.9 7.9 40.8-4.5 43.9-24.7l64-416c1.9-12.1-3.4-24.3-13.5-31.2s-23.3-7.5-34-1.4l-448 256zm52.1 25.5L409.7 90.6 190.1 336l1.2 1L68.2 285.7zM403.3 425.4L236.7 355.9 450.8 116.6 403.3 425.4z"/></svg> [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) &amp;  [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

<svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z"/></svg> (Katrien) Professor in insurance data science

<svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z"/></svg> (Jonas) PhD in insurance data science, now consultant in statistics, data science and data engineering with Data Minded

<svg aria-hidden="true" role="img" viewBox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9v28.1c0 28.4-10.8 57.7-22.3 80.8c-6.5 13-13.9 25.8-22.5 37.6C0 442.7-.9 448.3 .9 453.4s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7 .3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7C90.3 344.3 86 329.8 80 316.5V291.9c0-30.2 10.2-58.7 27.9-81.5c12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5 .8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1L624.2 182.6c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1C336.1 33.4 328.1 32 320 32zM128 408c0 35.3 86 72 192 72s192-36.7 192-72L496.7 262.6 354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6L143.3 262.6 128 408z"/></svg> (Roel) PhD in insurance data science, now consultant in data science with AI start up [Prophecy Labs](https://www.prophecylabs.com/)

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  
  ```r
  version$version.string
  ## [1] "R version 4.3.1 (2023-06-16 ucrt)"
  ```

☑ Do you have a fairly recent version of RStudio? 
  
  ```r
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```
  
☑ Do you have a fairly recent version of Python?


```r
library(reticulate)
reticulate::py_discover_config()
```

---

name: checklist (cont'd)

# Checklist (cont'd)

☑ Have you installed the R packages listed in the software requirements? 

☑ Did you go through the installation of Python libraries {tensorflow} and {keras}? 


```r
library(keras)
library(tensorflow)
path_to_python &lt;- "C:/Users/.../AppData/Local/Programs/Python/Python311/python.exe" 
# specify path to your Python version 
virtualenv_create("r-reticulate", python = path_to_python)
install_tensorflow(envname = "r-reticulate")
install_keras(envname = "r-reticulate")
use_virtualenv("r-reticulate")
```

or

☑ Have you created an account on posit cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this module .font140[<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M156.6 384.9L125.7 354c-8.5-8.5-11.5-20.8-7.7-32.2c3-8.9 7-20.5 11.8-33.8L24 288c-8.6 0-16.6-4.6-20.9-12.1s-4.2-16.7 .2-24.1l52.5-88.5c13-21.9 36.5-35.3 61.9-35.3l82.3 0c2.4-4 4.8-7.7 7.2-11.3C289.1-4.1 411.1-8.1 483.9 5.3c11.6 2.1 20.6 11.2 22.8 22.8c13.4 72.9 9.3 194.8-111.4 276.7c-3.5 2.4-7.3 4.8-11.3 7.2v82.3c0 25.4-13.4 49-35.3 61.9l-88.5 52.5c-7.4 4.4-16.6 4.5-24.1 .2s-12.1-12.2-12.1-20.9V380.8c-14.1 4.9-26.4 8.9-35.7 11.9c-11.2 3.6-23.4 .5-31.8-7.8zM384 168a40 40 0 1 0 0-80 40 40 0 1 0 0 80z"/></svg>]

--

* .KULbginline[de-mystify] neural networks

--

* develop foundations of working with (different types of) .KULbginline[neural networks]

--

* focus on the use of neural networks for the .KULbginline[analysis of claim frequency + severity data], also in combination with GLMs or tree-based ML models 

--

* discuss how to .KULbginline[evaluate] and .KULbginline[interpret] neural networks

--

* step from simple networks (for regression) to .KULbginline[auto encoders] and .KULbginline[convolutional] networks.

--



---

# Want to read more?

.pull-left[

This presentation is based on

* Michael A. Nielsen (2015) [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/)

* the work of prof. Taylor Arnold, in particular Chapter 8 in the book [A computational approach to statistical learning](https://www.routledge.com/A-Computational-Approach-to-Statistical-Learning/Arnold-Kane-Lewis/p/book/9780367570613) by Arnold, Kane &amp; Lewis (2019)

* Boehmke (2020) on [Deep Learning with R: using Keras with TensorFlow backend](https://github.com/rstudio-conf-2020/dl-keras-tf)

* our recent paper Holvoet, Antonio &amp; Henckaerts (2023, arxiv) on [Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff](https://arxiv.org/abs/2310.12671) with replication code on [GitHub](https://github.com/freekholvoet/nnforfreqsevpricing).

]

.pull-right[

Actuarial modelling with neural nets is covered in

* Wüthrich &amp; Buser (2020) [Data analytics for non-life insurance pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308), in particular Chapter 5

* Wüthrich (2019) [From Generalized Linear Models to neural networks, and back](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3491790)

* Wüthrich &amp; Merz (2019) [Editorial: Yes, we CANN!](https://www.cambridge.org/core/journals/astin-bulletin-journal-of-the-iaa/article/editorial-yes-we-cann/66E8BEC373B5CCEF3BF3303D442D6B75), in ASTIN Bulletin 49/1.

* Denuit, Hainaut &amp; Trufin (2019) [Effective Statistical Learning Methods for Actuaries: Neural Networks and Extensions](https://www.springer.com/gp/book/9783030258269), Springer Actuarial Lecture Notes

* A series of (working) papers covering the use of neural nets in insurance pricing (classic, and with telematics collected data), mortality forecasting, reserving, ... 

]

---

# Module 3's outline

.pull-left[

* [Getting started](#start)
  - Unpacking our toolbox
  - Tensors

* [De-mystifying neural networks](#demystify)
  - What's in a name?
  - A simple neural network
  
* [Neural network architecture](#fundamentals)
  - An architecture with layers in {keras}
  
* [Network compilation](#compilation)
  - Loss function and forward pass
  - Gradient descent and backpropagation
  - Performance metrics
  - Model evaluation
  


]

.pull-right[  

* [Regression with neural networks](#regression)
  - Redefining GLMs as a neural network
  - Including exposure
  - Case study

* [Convolutional neural networks](#cnn)
  - Handling new data formats
  - Convolutional layers explained
  - Evaluation and interpretation
  
* [Auto encoders](#autoencoder)
  - Data compression and feature extraction
  - Evaluation
  
]



---

name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

&lt;img src = "img/AI_ML_DL.jpg" height = "350px" /&gt;

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---
class: inverse, center, middle
name: start

# Getting started

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---

# What's the excitement about?

### .font140[<svg aria-hidden="true" role="img" viewBox="0 0 384 512" style="height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M297.2 248.9C311.6 228.3 320 203.2 320 176c0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9c3.7 5.3 8.1 11.3 12.8 17.7l0 0c12.9 17.7 28.3 38.9 39.8 59.8c10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5c-9.9-18-22.2-34.9-34.5-51.8l0 0 0 0c-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176C16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3c-5 7.2-10.2 14.3-15.4 21.4l0 0 0 0c-12.3 16.8-24.6 33.7-34.5 51.8c-5.9 10.8-9.6 22.5-11.8 34.5H226.4c2.6-18.7 7.9-38.6 18.3-57.5c11.5-20.9 26.9-42.1 39.8-59.8l0 0 0 0 0 0c4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80c8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80V416H272v16c0 44.2-35.8 80-80 80z"/></svg>] Neural networks are an exciting topic to explore, because:

--

* They are a .KULbginline[biologically-inspired programming paradigm] that enables a computer to learn from data.

--

* .KULbginline[Deep learning] is a powerful set of techniques for learning in neural networks.

--

* Neural networks and deep learning provide .KULbginline[best-in-class solutions] to many problems in image recognition, speech recognition and natural language processing.

--

* The .KULbginline[universal approximation theorem] (Hornik et al., 1989; Cybenko, 1989) states that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.

---

# The programming framework for today

&lt;br&gt;
&lt;img src="img/modelflow.png" width="600" height="100" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

* R: &lt;br&gt;The workshop's programming language, including an R interface to Python Keras and TensorFlow. 

* Keras: &lt;br&gt;An inuitive high level Python interface to TensorFlow.

* TensorFlow: &lt;br&gt; Open source platform for machine learning developed by the Google Brain Team, see [https://www.tensorflow.org/](https://www.tensorflow.org/). &lt;br&gt; Special focus on training deep neural networks.

---

# More on the programming framework

&lt;br&gt;
.center[
&lt;img src = "img/pytorch.png" height="200px" /&gt;
]
&lt;br&gt;

* PyTorch: &lt;br&gt;  machine learning framework based on the Torch library, used for tensor computing and deep learning with applications such as computer vision and natural language processing; originally developed by Meta AI, see [https://pytorch.org/](https://pytorch.org/). 

* In the Fall of 2023, Keras announced the release of [Keras Core](https://keras.io/keras_core/announcement/), making it possible to run Keras workflows on top of arbitrary frameworks, e.g. TensorFlow and PyTorch.

---

# R packages

Today's session will make extensive use of {keras}, {tensorflow} and {tidyverse}. &lt;br&gt;
Do not forget to load these packages in your R session.


```r
library(keras)
library(tensorflow)
library(tidyverse)
```

.center[
&lt;img src = "img/keras.jpg" height="200px" /&gt; &amp;nbsp; &amp;nbsp; 
&lt;img src = "img/tensorflow.png" height="200px" /&gt; &amp;nbsp; &amp;nbsp; 
&lt;img src = "img/tidyverse.png" height="200px" /&gt;
]

Link to the documentation: 

* [https://tensorflow.rstudio.com/](https://tensorflow.rstudio.com/)

---

# Why is this thing called TensorFlow?

A scalar is a single number, or a 0D tensor, i.e. .hi-pink[zero dimensional]:

&lt;!-- `$$\text{age_car = 5}, \quad \text{fuel = gasoline}, \quad \text{bm = 10}$$` --&gt;
.center[
`age_car = 5`, &amp;nbsp;&amp;nbsp; `fuel = gasoline`, &amp;nbsp;&amp;nbsp; `bm = 10`
]

In tensor parlance a scalar has 0 axes.

--

In a .hi-pink[big data world] with structured and unstructured data, our .hi-pink[input] can be a

* a single time series: 1-dimensional, with 1 axis

* one sound fragment: 2-dimensional, with 2 axes

* one image in color: 3-dimensional, with 3 axes

* one movie: 4-dimensional, with 4 axes

* ...

--

We require a framework that can flexibly adjust to all these data structures!

---

# Why is this thing called TensorFlow? (cont.)

.hi-pink[TensorFlow] is this flexible framework which consists of highly optimized functions based on .hi-pink[tensors].

What is a .hi-pink[tensor]?

* A 1-dimensional tensor is a vector (e.g. closing daily stock price during 250 days)

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-7-1.png" width="300" height="20" style="display: block; margin: auto;" /&gt;

--
* A 2-dimensional tensor is a matrix (e.g. a tabular data set with observations and features)

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-8-1.png" width="300" height="100" style="display: block; margin: auto;" /&gt;

* ...

--

Tensors generalize vectors and matrices to an arbitrary number of dimensions. 

Many matrix operations, such as the matrix product, can be generalized to tensors.

Luckily Keras provides a high level interface to TensorFlow, such that we will have only minimal exposure to tensors and the complicated math behind them.


---

# Example of a 3D tensor

.pull-left[

Let's picture a stock price dataset where

* each minute we record the current price, lowest price and highest price
* a trading day has 390 minutes and a trading year has 250 days.

Then, one year of data can then be stored in a 3D tensor `(samples, timesteps, features)`, here: `(250, 390, 3)`.

]

.pull-right[

.center[
&lt;img src = "img/3D_tensor.png" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Example of a 4D tensor

.pull-left[

Let's picture an image data set where

* each image has a specific height and width
* three color channels (Red, Green, Blue) are registered
* multiple images (`samples`) are stored.


Then, a collection of images can be stored in a 4D tensor `(samples, height, width, channels)`.

]

.pull-right[

.center[
&lt;img src = "img/4D_tensor.png" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Example of a 5D tensor

.pull-left[

Let's picture a video data set where

* each video sample is one minute long and has a number of frames per second (e.g. 4 frames per second)
* each frame has a specific height (e.g. 256 pixels) and width (e.g. 144 pixels)
* three color channels (Red, Green, Blue)
* multiple images (`samples`) are stored.


Then, a collection of images can be stored in a 4D tensor `(samples, frames, height, width, channels)` which becomes here `(samples, 240, 256, 144, 3)`.


]

.pull-right[

.center[
&lt;img src = "img/5D_tensor.jpg" width="500px" /&gt;
]

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]

]

---

# Tensor functions

.pull-left[
Keras generalizes common R functions for inputs of type tensor. These functions can be recognized by the prefix `k_`.

A good introduction to working with tensors is [https://tensorflow.rstudio.com/guides/tensorflow/basics](https://tensorflow.rstudio.com/guides/tensorflow/basics).

* `k_constant`: create and initialise a tensor. 

```r
x &lt;- k_constant(c(1, 2, 3, 4, 5, 6), 
                shape = c(3, 2))
x
## tf.Tensor(
## [[1. 2.]
##  [3. 4.]
##  [5. 6.]], shape=(3, 2), dtype=float32)

x$shape
## TensorShape([3, 2])
x$dtype
## tf.float32
```
]

.pull-right[
Most tensor operations require an axis parameter to specify the dimensions over which the function should be performed.


See the [Keras documentation](https://tensorflow.rstudio.com/reference/keras/#backend) for a list of all tensor functions. 

* `k_mean`: calculate the mean of the tensor.


```r
k_mean(x, axis = 1)
## tf.Tensor([3. 4.], shape=(2), dtype=float32)
```


```r
k_mean(x, axis = 2)
## tf.Tensor([1.5 3.5 5.5], shape=(3), dtype=float32)
```

]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

&lt;br&gt;
In this warming up exercise you .KULbginline[create a tensor] and .KULbginline[apply basic tensor functions].
&lt;br&gt;
&lt;br&gt;
* .hi-pink[Q.1]: create a 3-dimensional tensor in R with values `1, 2, ..., 12` and shape `(2, 3, 2)`.

* .hi-pink[Q.2]: calculate the logarithm of this tensor.

* .hi-pink[Q.3]: calculate the mean of this tensor over the third axis. 
]

---

class: clear

.pull-left[
.hi-pink[Q.1]: create a tensor

```r
x &lt;- k_constant(1:12, shape = c(2, 3, 2))
x
## tf.Tensor(
## [[[ 1.  2.]
##   [ 3.  4.]
##   [ 5.  6.]]
## 
##  [[ 7.  8.]
##   [ 9. 10.]
##   [11. 12.]]], shape=(2, 3, 2), dtype=float32)
```

.hi-pink[Q.2]: calculate the logarithm 

```r
k_log(x)
## tf.Tensor(
## [[[0.        0.6931472]
##   [1.0986123 1.3862944]
##   [1.609438  1.7917595]]
## 
##  [[1.9459102 2.0794415]
##   [2.1972246 2.3025851]
##   [2.3978953 2.4849067]]], shape=(2, 3, 2), dtype=float32)
```
]

.pull-right[
`log(x)` would have also resulted in the correct answer.&lt;br&gt;However, it is best practice to use `k_log`, since the R-function `log` can not be evaluated within TensorFlow:

```r
library(tensorflow)
tf$\`function`(k_log)(x)
```

```
## tf.Tensor(
## [[[0.        0.6931472]
##   [1.0986123 1.3862944]
##   [1.609438  1.7917595]]
## 
##  [[1.9459102 2.0794415]
##   [2.1972246 2.3025851]
##   [2.3978953 2.4849067]]], shape=(2, 3, 2), dtype=float32)
```


```r
tf$\`function`(log)(x)
```

```
## Error in py_call_impl(...) : 
##  Unable to convert R object to Python type
```

]

---

class: clear

.pull-left[
.hi-pink[Q.3]: calculate the mean along the third axis

```r
k_mean(x, axis = 3)
## tf.Tensor(
## [[ 1.5  3.5  5.5]
##  [ 7.5  9.5 11.5]], shape=(2, 3), dtype=float32)
```
... and explore the other axes as well
]

.pull-right[
.hi-pink[Q.3]: mean along the first axis

```r
k_mean(x, axis = 1)
## tf.Tensor(
## [[4. 5.]
##  [6. 7.]
##  [8. 9.]], shape=(3, 2), dtype=float32)
```

or the second axis

```r
k_mean(x, axis = 2)
## tf.Tensor(
## [[ 3.  4.]
##  [ 9. 10.]], shape=(2, 2), dtype=float32)
```
]

---

class: inverse, center, middle
name: data-sets

# Data sets used in the course 

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

name: data-sets-used

# Data sets used in this course - MTPL &lt;img src="img/pipe.png" class="title-hex"&gt; &lt;img src="img/dplyr.png" class="title-hex"&gt; &lt;img src="img/ggplot2.png" class="title-hex"&gt;

We will (once again) use the Motor Third Party Liability data set. There are 163,231 policyholders in this data set. 

The frequency of claiming (`nclaims`) and corresponding severity (`avg`, the amount paid on average per claim reported by a policyholder) are the .KULbginline[target variables] in this data set. 

Predictor variables are: 

* the exposure-to-risk, the duration of the insurance coverage (max. 1 year)
* factor variables, e.g. gender, coverage, fuel
* continuous, numeric variables, e.g. age of the policyholder, age of the car
* spatial information: postal code (in Belgium) of the municipality where the policyholder resides.

More details in [Henckaerts et al. (2018, Scandinavian Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#data-driven) and [Henckaerts et al. (2021, North American Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#tree-based-pricing).

---

class: clear, center, middle

background-image: url("img/MnistExamples.png")
background-size: cover
background-size: 95% 
background-position: left

.font1000.bold[MNIST]

---

name: data-sets-used

# Data sets used in this course - MNIST

As discussed, not all data are in tabular format. 

We analyze an .KULbginline[image database] from the Modified National Institute of Standards and Technology, short [MNIST](https://en.wikipedia.org/wiki/MNIST_database). 

Working with MNIST will learn us how machine learning methods can be used to work with new data sources, such as images. 

.pull-left[
* Large database of 70,000 labeled images of handwritten digits, see [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)

* Images are preprocessed, i.e. scaled and centered.

* Classic test case for machine learning classification algorithms. Current models achieve an accuracy of [more than 99.5%](https://en.wikipedia.org/wiki/MNIST_database). 
]

.pull-right[
.center[
&lt;img src = "img/neural_network_sample.gif" height = "350px" /&gt;
]
]


---

# Data sets used in this course - MNIST




.pull-left[

The images are in grayscale. Each image is stored as a 28x28 intensity matrix, with intensity expressed on a scale from 0-255.

&lt;img src="ML_part3_files/figure-html/example_mnist_input-1.png" width="360" height="360" style="display: block; margin: auto;" /&gt;
]

.pull-right[
Recognizing that the images below all represent the digit 8 is trivial for humans, but difficult for computers.

&lt;img src="ML_part3_files/figure-html/plot_example_mnist-1.svg" height="150" style="display: block; margin: auto;" /&gt;

Neural networks are ideal for situations where the relation between the input (here: intensity matrix) and the output (here: 0-9) is complicated.
]

---

# Loading the MNIST dataset

.pull-left[

`keras::dataset_mnist()` retrieves the MNIST dataset from the online repository. &lt;br&gt; Alternatively, the dataset can be loaded from the course material directory.

```r
# download the dataset from the online repository
mnist &lt;- keras::dataset_mnist()
# load the dataset from the course files
load('../data/mnist.RData')
```

Assign new names to the input and output data.

```r
input &lt;- mnist$train$x
output &lt;- mnist$train$y
test_input &lt;- mnist$test$x
test_output &lt;- mnist$test$y
```
]

.pull-right[
The input data is an `array`:

```r
class(input)
```


```
## [1] "array"
```

with 60,000 28x28 images (the training data):

```r
dim(input)
```


```
## [1] 60000    28    28
```

Select the first image:

```r
input[1, , ]
```
]

---

class: inverse, center, middle
name: demystify

# De-mystifying neural networks

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# What's in a name?

.pull-left[
Different types of neural networks and their applications: 

* .KULbginline[ANN]: Artificial Neural Network &lt;br&gt; for regression and classification problems, with vectors as input data

* .KULbginline[CNN]: Convolutional Neural Network &lt;br&gt; for image processing, image/face/... recognition, with images as input data

* .KULbginline[RNN]: Recurrent Neural Network &lt;br&gt; for sequential data such as text or time series

... and many more!
]

.pull-right[

&lt;div style='height:100%; overflow:scroll;'&gt;
&lt;img src = "img/neural_network_types.png" /&gt;
&lt;/div&gt;

]
---

# A simple neural network

.pull-left[

De-mystify artificial neural networks (ANNs): 

* a collection of inter-woven linear models
* extending linear approaches to detect .KULbginline[non-linear] interactions in .KULbginline[high-dimensional] data.

See the picture on the right.

.KULbginline[Goal]: predict a scalar response `\(y\)` from scalar input `\(x\)`.

]

.pull-right[



&lt;img src = "img/a_simple_neural_network.png" /&gt;

Some terminology: 

* `\(x\)` is the .KULbginline[input layer]
* `\(v\)` is the .KULbginline[output layer]
* middle layer is a .KULbginline[hidden layer]
* four neurons: `\(x\)`, `\(z_1\)`, `\(z_2\)` and `\(v\)`.


]

---

# A simple neural network (cont.)

.pull-left[

First, we apply two independent .KULbginline[linear models]:
$$
`\begin{eqnarray*}
z_1 &amp;=&amp; b_1 + x \cdot w_1 \\
z_2 &amp;=&amp; b_2 + x \cdot w_2
\end{eqnarray*}`
$$
using four parameters: two intercepts and two slopes.

Next, we construct .KULbginline[another linear model] with the `\(z_j\)` as inputs:
$$
`\begin{eqnarray*}
\hat{y} := v &amp;=&amp; b_3 + z_1 \cdot u_1 + z_2 \cdot u_2.
\end{eqnarray*}`
$$
Putting it all together:
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 +  z_1 \cdot u_1 + z_2 \cdot u_2 \\
&amp;=&amp; b_3 + (b_1 + x \cdot w_1)\cdot u_1 + (b_2+ x \cdot w_2) \cdot u_2 \\
&amp;=&amp; (b_3 + u_1 \cdot b_1+ u_2 \cdot b_2) + (w_1 \cdot u_1 + w_2 \cdot u_2) \cdot x \\
&amp;=&amp; \text{(intercept)} + \text{(slope)} \cdot x. 
\end{eqnarray*}`
$$

]


.pull-right[

Model is over-parametrized, with infinitely many ways to describe the same model. 

Essentially, still a linear model!

&lt;img src = "img/sum_linear_models.png" /&gt;

]

---

# A simple neural network (cont.)

.pull-left[
We capture .KULbginline[non-linear] relationships between `\(x\)` and `\(v\)` by replacing
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 + z_1 \cdot u_1 + z_2 \cdot u_2.
\end{eqnarray*}`
$$
with
$$
`\begin{eqnarray*}
v &amp;=&amp; b_3 + \sigma(z_1) \cdot u_1 + \sigma(z_2) \cdot u_2 \\
&amp;=&amp; b_3 + \sigma(b_1 + x \cdot w_1) \cdot u_1 + \sigma(b_2 + x \cdot w_2) \cdot u_2,
\end{eqnarray*}`
$$
where `\(\sigma(.)\)` is an .KULbginline[activation function], a mapping from `\(\mathbb{R}\)` to `\(\mathbb{R}\)`.


Adding an activation function greatly increases the .KULbginline[set of possible relations] between `\(x\)` and `\(v\)`!
]

.pull-right[
For example, the rectified linear unit (ReLU) activation function:
$$
`\begin{eqnarray*}
\text{ReLU}(x) &amp;=&amp; \begin{cases} \begin{array}{cc}
                                   x, &amp; \text{if}\ x \geq 0 \\
                                   0, &amp; \text{otherwise}.
                                 \end{array}
                    \end{cases}
\end{eqnarray*}`
$$

&lt;img src = "img/sum_ReLU.png" /&gt;

Many more activation functions: sigmoid, softmax, identity, etc. (see further).
]

---

# Examples of activation functions

&lt;img src="img/activations.jpg" width="250%" height="250%" style="display: block; margin: auto;" /&gt;

Source: [https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning)

---

# From the simple neural network to ANNs

.pull-left[
Artificial Neural networks (.KULbginline[ANNs]): 

* a collection of neurons 
* organized into an ordered set of layers 
* directed connections pass signals between neurons in adjacent layers 
* .hi-pink[to train]: &lt;br&gt; update parameters describing the connections by minimizing loss function over training data 
* .hi-pink[to predict]: &lt;br&gt; pass `\(\boldsymbol{x}_i\)` to first layer, output of final layer is `\(\hat{y}_i\)`.

The network is .KULbginline[dense] or .KULbginline[densely connected] if each neuron in a layer receives an input from all the neurons present in the previous layer.



]

.pull-right[

&lt;img src = "img/feed_forward.png"/&gt;

This is a .KULbginline[feedforward] neural network - no loops!

]

---

# The neural nets' terminology

.pull-left[
Using the neural nets terminology or language: 

* intercept called .KULbginline[the bias]

* slopes called .KULbginline[weights] 

* `\(L+1\)` layers in total, with input layer denoted as layer 0 and output layer as `\(L\)`

* use `\(a(.)\)` (from .KULbginline[activation]) or `\(\sigma(.)\)` to denote the output of a given neuron in a given layer

* technically, .KULbginline[deep learning] refers to any neural network that has 
2 or more hidden layers.

]

.pull-right[

&lt;img src = "img/perceptron.png"/&gt;


A single layer ANN, also called perceptron or artificial neuron.

]

---

class: inverse, center, middle
name: fundamentals

# Neural network architecture in Keras

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# Preparing the MNIST data for an ANN

.pull-left[

Let's construct a densely connected, feed forward neural network for the multi-class classification problem in MNIST.

We .KULbginline[flatten] the image data (28x28 matrix) into a vector of length 784:


```r
input &lt;- tensorflow::array_reshape(input,
                    c(nrow(input), 28*28)) / 255

test_input &lt;- tensorflow::array_reshape(test_input,
                    c(nrow(test_input), 28*28)) / 255
```

Later in this course we will see how we can analyze this data .hi-pink[without flattening]!

]

.pull-right[

We construct 10 dummy variables (0-9) for the output of the model:


```r
output &lt;- keras::to_categorical(output, 10)
```



```r
test_output &lt;- keras::to_categorical(test_output, 10)
```




]

---

# Preparing the MNIST data for an ANN (cont.)

.pull-left[

The R-script includes a function `plot_image` to visualize the input:


```r
plot_image(input[17, ], legend = FALSE)
```

]

.pull-right[

&lt;img src="ML_part3_files/figure-html/plot_image_function-1.png" style="display: block; margin: auto;" /&gt;

]

---

# An architecture with layers

In a neural network, **.green[input]** travels through a sequence of **.blue[layers]**, 
and gets transformed into the **.red[output]**.

.pull-left[
&lt;img src="img/example_neural_network.png" width="400" height="250" style="display: block; margin: auto;" /&gt;
]

.pull-right[
This sequential layer structure is really at the core of the Keras library.


```r
model &lt;- 
* keras_model_sequential() %&gt;%
  layer_dense(...) %&gt;%
  layer_dense(...)
```
]

&lt;br&gt;

.KULbginline[Layers] consist of .KUlbginline[nodes] and the .KULbginline[connections] between these nodes and the previous layer.

`layer_dense()` is creating a fully connected feed forward neural network.

---

# An architecture with layers (cont.)

.pull-left[


```r
model &lt;- keras_model_sequential() %&gt;%
* layer_dense() %&gt;% # hidden layer
* layer_dense() # output layer
```

Each `layer_dense()` represents a hidden layer *or* the final output layer.


```r
model &lt;- keras_model_sequential() %&gt;%
* layer_dense() %&gt;% # hidden layer 1
* layer_dense() %&gt;% # hidden layer 2
* layer_dense() %&gt;% # hidden layer 3
* layer_dense() # output layer
```

]

.pull-right[

* We can add multiple hidden layers by adding more `layer_dense()` functions.

* Technically, .KULbginline[deep learning] refers to any neural network that has 
2 or more hidden layers.

* The last `layer_dense()` will always represent the output layer.

]

---

# A hidden layer


```r
model &lt;- keras_model_sequential() %&gt;%
*        layer_dense(units = 512, activation = 'relu', input_shape = c(784)) # hidden layer
```

.pull-left[

* `units = 512`: number of nodes in the given layer

* `input_shape = c(784)`
   - tells the first hidden layer how many input features there are
   - only required for the first `layer_dense`
   
* `activation = 'relu'`: this hidden layer uses the ReLU activation function. 

Here: the MNIST pictures (28x28) are flattened to a an input vector of length 784.


]

.pull-right[

&lt;img src="img/hidden_layer.png" width="715" style="display: block; margin: auto;" /&gt;

]

---

# A hidden layer - some intuition

Nodes in the hidden layer(s) represent intermediary features that we do not explicitely define. 

We let the model decide the optimal features.

For example, recognizing a digit is more difficult than recognizing a horizontal or vertical line.

&lt;img src="img/neural_network_digital.png" width="700" height="100" style="display: block; margin: auto;" /&gt;

Hidden layers automatically split the problem into smaller problems that are easier to model.

&lt;img src="img/neural_network_digital_example.png" width="700" height="100" style="display: block; margin: auto;" /&gt;

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`:


]

.pull-right[

&lt;img src="img/output_layer_continuous.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`
   - binary classification: `units = 1`
   
]

.pull-right[




&lt;img src="img/output_layer_binary.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
   - regression: `units = 1`
   - binary classification: `units = 1`
   - multi-class classification: `units = n`
]
.pull-right[

&lt;img src="img/output_layer_multi.png" width="75%" height="75%" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
]
.pull-right[

&lt;img src="img/activation_identity.png" width="804" style="display: block; margin: auto;" /&gt;

]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
   - binary classification: `activation = 'sigmoid'`
]
.pull-right[
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;" /&gt;

`\(f(y) = \frac{1}{1 + e^{-y}}\)`
]

---

# Output layer


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 512, activation = 'relu', input_shape = c(784) %&gt;%
* layer_dense(units = 10, activation = 'softmax')
```

The choice of the `units` and `activation` function in the output layer depend on the type of prediction!

.pull-left[
Two primary arguments of concern for the final output layer:

1. number of units
2. activation function
   - regression: `activation = NULL` (identity function)
   - binary classification: `activation = 'sigmoid'`
   - multi-class classification: `activation = 'softmax'`

]

.pull-right[

&lt;img src="img/softmax.png" width="800" style="display: block; margin: auto;" /&gt;

]


---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

&lt;br&gt;
Ultimately, here is a summary of the network architecture discussed so far for the MNIST data


```r
model &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 512, 
              activation = 'relu', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 10, 
              activation = 'softmax')
  
```

Can you figure out how many parameters will be trained for this network?

]

---

class: clear

.pull-left[


```
## Model: "sequential"
## ________________________________________________________________________________
##  Layer (type)                       Output Shape                    Param #     
## ================================================================================
##  dense_1 (Dense)                    (None, 512)                     401920      
##  dense (Dense)                      (None, 10)                      5130        
## ================================================================================
## Total params: 407050 (1.55 MB)
## Trainable params: 407050 (1.55 MB)
## Non-trainable params: 0 (0.00 Byte)
## ________________________________________________________________________________
```

]

.pull-right[

The model has 407,050 parameters:

* 784 inputs (28x28 pixels in a single image) 

* 1 hidden layer, with
  - 512 nodes and ReLU activation
  - thus, (784 x 512) + 512 = 401,920 parameters

* multi-class output layer, with
  - 10 nodes
  - softmax activation function
  - thus, (512 x 10) + 10 = 5,130 parameters
  
* all together, that makes 407,050 parameters!

]


---


class: inverse, center, middle
name: compilation

# Network compilation

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;


---

# Loss function and forward pass

.pull-left[

* Initialize weights (randomly).

* The forward pass then results in predicted values `\(\hat{\textbf{y}}\)`, to be compared with `\(\textbf{y}\)`.

* The difference is measured with a loss function, the quantity that will be minimized during training.

Keras includes many .KULbginline[common loss functions]:

* `"mse"`: Gaussian
* `"poisson"`: Poisson
* `"binary_crossentropy"`: binary classification
* `"categorical_crossentropy"`: multi-class classification
* many others, see the [Keras documentation](https://keras.io/losses/)

Pick a loss function that aligns best to the problem at hand!
]

.pull-right[

&lt;img src="img/forward_pass3.png" width="561" style="display: block; margin: auto;" /&gt;

]

---

# Compiling the model

.pull-left[

```r
model &lt;- model %&gt;%
* compile(loss = "categorical_crossentropy",
          optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```


With `loss = "categorical_crossentropy"` the loss of a single training observation is 

$$ \sum_{j=0}^9 -y_j \cdot \log{(f(s_j))},   $$
where `\(j\)` runs over the classes in the multi-class prediction problem, `\(f(s_j)\)` is the fitted probability of class `\(j\)` and `\(y_j\)` is a 0/1 hot-encoding of the truly observed class.

For instance, when the true input digit is 1 the vector `\(y\)` is `\((0, 1, 0, 0, \ldots, 0)\)`.

]

.pull-right[

You can also define your  .hi-pink[own loss] function in Keras, e.g.

```r
mse &lt;- function(y_true, y_pred) { 
  k_mean((y_true - y_pred)^2, axis = 2) 
} 

model &lt;- model %&gt;%
* compile(loss = mse,
          optimizer = optimizer_rmsprop(),
          metrics = c('accuracy'))
```

* `k_mean` is the keras implementation of `mean` that takes a tensor as input.

* `axis = 2` calculates the mean over the different output nodes.
]

---
# Compiling the model (cont.)

.pull-left[

```r
model &lt;- model %&gt;%
  compile(loss = "categorical_crossentropy", 
*         optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```


Keras includes several .KULbginline[optimizers] for minimizing the loss function. 

Popular choices are:
* `optimizer_rmsprop()`
* `optimizer_adam()`
* other optimizers, see the [Keras documentation](https://keras.io/optimizers/).

The goal is to find weights and bias terms that .KULbginline[minimize the loss function].

]

.pull-right[



&lt;img src="img/forward_pass4.png" width="568" style="display: block; margin: auto;" /&gt;


]

---

# Three variants of gradient descent

.pull-left[

With .KULbginline[batch] gradient descent: 

* compute loss for each observation in the training data
* update parameters after all training examples have been evaluated
* .hi-pink[con]: scales horribly to bigger data sets.

With .KULbginline[stochastic] gradient descent: 

* randomly select an observation, compute gradient
* update parameters after this single observation has been evaluated
* .hi-pink[con]: takes a long time to convergence.
]

.pull-right[

With .KULbginline[mini-batch] gradient descent: 

* randomly select a subset of the training observations, compute gradient
* update parameters after this subset has been evaluated.

.KULbginline[Pros]: 

* balance efficiency of batch vs stochastic
* balance robust convergence of batch with some stochastic nature to avoid local minima.

.hi-pink[Cons]: 

* additional tuning parameter. 

]

---

# Fitting the model (cont.)

`fit(.)` tunes the model parameters (the weights and bias terms). 

.pull-left[


```r
model %&gt;%
  fit(input, 
      output, 
      batch_size = 128, 
      epochs = 10,
*     validation_split = 0.2)
```



]

.pull-right[

With the `validation_split = 0.2` we use the last 20% of our input training data as a hold-out validation set.

We evaluate the loss on this validation set at the end of each epoch.

]

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
&lt;br&gt;
You will now .KULbginline[design], .KULbginline[compile] and .KULbginline[fit] your own neural network for the MNIST dataset. 


As a form of paralellized model selection, all of us will play with different model parameters. This way we gain insight into which parameter values work well for this dataset.
&lt;br&gt;
&lt;br&gt;
**Base model**: the neural network with a single hidden layer, as specified in the R script.
&lt;br&gt;
&lt;br&gt;
Try some of the following ideas to improve the model: (more ideas on the next slide!)

* .KULbginline[add hidden layers]: the number of nodes in subsequent layers should decrease

* .KULbginline[change batch size]

* .KULbginline[change the activation function].
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

Examples of .KULbginline[layer types]

* [`layer_gaussian_noise`](https://keras.io/layers/noise/): adds gaussian noise N(0, stddev) to the nodes when training the model. This reduces the probability of overfitting.

```r
model &lt;- model %&gt;%
  layer_gaussian_noise(stddev)
```

* [`layer_dropout`](https://keras.io/layers/core/): sets a fraction `rate` of the input units to zero. This reduces the probability of overfitting.

```r
model &lt;- model %&gt;%
  layer_dropout(rate)
```

* [`layer_batch_normalization`](https://keras.io/layers/normalization/): centers and scales the values of each node in the previous layer.

```r
model &lt;- model %&gt;%
  layer_batch_normalization()
```


]

---

class: clear

.pull-left[

Several methods proposed to reduce overfitting:

* try different weight initializations
* early stopping
* regularization
* dropout.

Available in {keras} as: 


```r
layer_dense(units = , activation = "relu", 
      kernel_regularizer = regularizer_l2(l = 0.001))
```


```r
layer_dropout(0.6)
```


```r
# fit with callbacks
model %&gt;% fit(___, ___, callbacks = list(
  callback_early_stopping(___)
))
```

Fore more details, please consult [https://keras.rstudio.com/articles/training_callbacks.html](https://keras.rstudio.com/articles/training_callbacks.html).

]

.pull-right[

With .KULbginline[early stopping]:
* calculate validation performance after each epoch
* stop when this no longer improves.

With .KULbginline[regularization] *(cfr. lasso and {glmnet})*: e.g.
`$$\min_{w,b} \mathcal{L}_{w,b}(.,.)+\frac{\lambda}{2}\cdot \|w\|_2^2.$$`

With .KULbginline[dropout]:
* randomly set activations to zero, with fixed probability `\(p\)`
* both in forward propagation as well as backpropagation
* only in training, all nodes turned on during prediction.

]

---
# Model evaluation

`evaluate(.)` calculates losses and metrics on the test dataset. 

```r
model %&gt;% 
  evaluate(test_input, test_output, verbose = 0)
##      loss  accuracy 
## 0.2336413 0.9341000
```

`predict(.)` returns a vector of length 10 with the probability per output node.

```r
prediction &lt;- model %&gt;% 
  predict(test_input)
## 313/313 - 0s - 264ms/epoch - 843us/step

round(prediction[1, ], 3)
##  [1] 0.000 0.000 0.001 0.003 0.000 0.000 0.000 0.995 0.000 0.001
```

The predicted category is the node with the highest probability.

```r
category &lt;- apply(prediction, 1, which.max) - 1
actual_category &lt;- apply(test_output, 1, which.max) - 1
```


---


# Model evaluation (cont.)

We inspect the misclassified images to gain more insight in the model.

Below we show some examples for our pre-trained MNIST neural network.

.pull-left[

```r
head(which(actual_category != category))
## [1]  9 34 39 64 88 93
```


```r
index &lt;- 9
plot_image(test_input[index, ]) +
  ggtitle(paste(
    'actual: ', actual_category[index], 
    ' predicted: ', category[index], sep='')) +
  theme(legend.position = 'none', 
        plot.title = element_text(hjust = 0.5))
```
]

.pull-right[

&lt;img src="img/neural_network_misclassification.png" width="700" height="300" style="display: block; margin: auto;" /&gt;

]
---
# Model evaluation (cont.)

We inspect the images for which the model assigns the lowest probability to the correct class.


```r
# select per row, the probability corresponding to the correct class
prob_correct &lt;- prediction[cbind(1:nrow(prediction), actual_category+1)]

# get the index of the 5 lowest records in prob_correct
which(rank(prob_correct) &lt;= 5)
## [1] 1501 5735 6167 6506 6652
```

&lt;img src="img/neural_network_worst.png" width="700" height="300" style="display: block; margin: auto;" /&gt;
---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[
&lt;br&gt;
You will now .KULbginline[evaluate] your own model!
&lt;br&gt;&lt;br&gt;
* .hi-pink[Q.1]: calculate the accuracy of your model on the test set. &lt;br&gt; &lt;br&gt;
* .hi-pink[Q.2]: visualize some of the misclassified images from your model. &lt;br&gt;&lt;br&gt;
* .hi-pink[Q.3]: generate an image consisting of random noise and let the model classify this image. What do you think of the results? &lt;br&gt;
Remember: your input should be a 1x784 matrix with values in [0, 1].
]

---
# Feeding random data to a neural network

.pull-left[

```r
random &lt;- matrix(runif(28^2), nrow = 1)
plot_image(random[1, ])
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-78-1.png" width="360" height="360" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
round(predict(model, random), 3)
## 1/1 - 0s - 19ms/epoch - 19ms/step
##      [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
## [1,]    0    0 0.617 0.124 0.002 0.196 0.042 0.001 0.018     0
```

Our pre-trained MNIST model is pretty sure that the input on the left is a two!
]

---
# Model understanding

Inspecting the calibrated weights can provide some insight in the features created in the hidden layer.

.pull-left[
Every node in the first hidden layer has 784 connections with the input layer. 

The weights of these connections can be visualized as an 28x28 image.


```r
node &lt;- 9
layer &lt;- 1
weights &lt;- model$get_weights()[[2*(layer-1) + 1]][, node]

plot_image(as.numeric(weights))
```

On the right we show a visualization of the calibrated weights for a pre-trained model with (only) 16 nodes in the first hidden layer.
]

.pull-right[

&lt;img src="img/neural_network_calibrated_weights.png" width="400" height="400" style="display: block; margin: auto;" /&gt;
]
---
# Summary of the fundamentals

.pull-left[

We discussed so far: 

* design neural networks .KULbginline[sequentially] in {keras}
`keras_model_sequential` 

* layers consist of .KULbginline[nodes] and .KULbginline[connections]

* vanilla choice is a .KULbginline[fully connected layer]
`layer_dense`

* .KULbginline[fit] the model via gradient descent (i.e. backpropagation).
]

.pull-right[

List of .KULbginline[tuning/architectural] choices:

  - the number of layers
  - the number of nodes per layer
  - the activation functions
  - the layer type *(more on this coming soon)*
  - the loss function
  - the optimization algorithm
  - the batch size
  - the number of epochs
  - ...
]

---

class: clear

&lt;div style='height:100%; overflow:scroll;'&gt;
&lt;img src = "img/neural_network_types.png" /&gt;
&lt;/div&gt;

---

class: inverse, center, middle
name: regression

# Claim frequency and severity regression

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---
# Preparing the MTPL data
We will now predict .KULbginline[claim frequency] and .KULbginline[severity] in the MTPL data with a .KULbginline[neural network].

Load the .hi-pink[MTPL] data:

```r
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) 
mtpl &lt;- read.table('../data/PC_data.txt',
                   header = TRUE, stringsAsFactors = TRUE) %&gt;% 
  as_tibble() %&gt;% rename_all(tolower) %&gt;% rename(expo = exp)
```

Create a .hi-pink[training] and .hi-pink[test] set with {rsample}:

```r
library(rsample)
set.seed(54321)
data_split &lt;- initial_split(mtpl)
mtpl_train &lt;- training(data_split)
mtpl_test  &lt;- testing(data_split)
# Reshuffling of the training observations
mtpl_train &lt;- mtpl_train[sample(nrow(mtpl_train)), ]
```

---
# Regression with neural networks

In Module 1 we fitted .KULbginline[GLMs] for .hui-pink[claim frequency] as follows:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{x}^{'}\color{#20B2AA}{\beta})).$$
&lt;br&gt;

We now .hi-pink[redefine] this model as a .KULbginline[neural network]:

.center[
Formula |  GLM  | Neural network
------------- | -------------
`\(\color{#FFA500}{Y}\)`   | response | output node
&amp;nbsp; Poisson &amp;nbsp; | &amp;nbsp; distribution &amp;nbsp; | loss function
exp            | inverse link function | &amp;nbsp; activation function &amp;nbsp;
`\(\color{#e64173}{x}\)`  | predictors | input nodes
`\(\color{#20B2AA}{\beta}\)` | regression parameters | weights
]

---
# Your first claim frequency neural network

Let's start with a model with .KULbginline[only an intercept]:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{1} \cdot \color{#20B2AA}{\beta})).$$

.pull-left[

```r
nn_freq_intercept &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

.hi-pink[Q.]: How many parameters does this model have?
]

.pull-right[
* `layer_dense`: there are .hi-pink[no hidden layers], the input layer is directly connected to the output layer.

* `units = 1`: there is .hi-pink[one] output node.

* `activation = 'exponential'`: we use an .hi-pink[exponential] inverse link function.

* `input_shape = c(1)`: there is .hi-pink[one] input node, i.e., the intercept which will be constant one.

* `use_bias = FALSE`: we don't need a .hi-pink[bias] term, since we explicitly include an input node equal to one. 

* `loss = 'poisson'`: we maximize the .hi-pink[Poisson] likelihood, i.e., minimize the Poisson deviance.
]

---
# Your first claim frequency neural network

Let's start with a model with .KULbginline[only an intercept]:

$$ \color{#FFA500}{Y} \sim \texttt{Poisson}(\lambda = \exp( \color{#e64173}{1} \cdot \color{#20B2AA}{\beta})).$$

.pull-left[

```r
nn_freq_intercept &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

.hi-pink[Q.]: How many parameters does this model have?

```r
nn_freq_intercept$count_params()
## [1] 1
```

]

.pull-right[
* `layer_dense`: there are .hi-pink[no hidden layers], the input layer is directly connected to the output layer.

* `units = 1`: there is .hi-pink[one] output node.

* `activation = 'exponential'`: we use an .hi-pink[exponential] inverse link function.

* `input_shape = c(1)`: there is .hi-pink[one] input node, i.e., the intercept which will be constant one.

* `use_bias = FALSE`: we don't need a .hi-pink[bias] term, since we explicitly include an input node equal to one.  

* `loss = 'poisson'`: we maximize the .hi-pink[Poisson] likelihood.
]

---
# Your first claim frequency neural network (cont.)

.pull-left[
Create .hi-pink[vectors] for the input and output:


```r
intercept &lt;- rep(1, nrow(mtpl_train))

counts &lt;- mtpl_train$nclaims
```

.KULbginline[Fit] the neural network:

```r
nn_freq_intercept %&gt;% fit(x = intercept,
                          y = counts,
                          epochs = 30,
                          batch_size = 1024, 
                          validation_split = 0,
                          verbose = 0)
```



]

.pull-right[
* `x = intercept`: use the intercept as .hi-pink[feature].

* `y = counts`: use the claim counts as .hi-pink[target].

* `epochs = 20`: perform 20 training .hi-pink[iterations] over the complete data.

* `batch_size = 1024`: use .hi-pink[batches] with 1024 observations to update weights.

* `validation_split = 0`: don't use a .hi-pink[validation] set, so all observations are used for training. 

* `verbose = 0`: .hi-pink[silence] keras such that no output is generated during fitting.
]

---
# Comparing our neural network with a GLM

We .KULbginline[compare] the results of our neural network with the same model specified as a GLM:


```r
glm_freq_intercept &lt;- glm(nclaims ~ 1,
                          data = mtpl_train,
                          family = poisson(link = 'log'))

# GLM coefficients
glm_freq_intercept$coefficients
## (Intercept) 
##   -2.091133

## NN weights
nn_freq_intercept$get_weights()
## [[1]]
##           [,1]
## [1,] -2.085138
```

There is a small difference in the parameter estimate, resulting from a .hi-pink[different optimization technique].


---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

We have shown that a .KULbginline[Poisson GLM] can be implemented as a neural network.
&lt;br&gt;

* .hi-pink[Q.1]: adapt this code to replicate a .KULbginline[binomial GLM] with a .hi-pink[logit] link function. Add .KULbginline[accuracy] as a .hi-pink[metric] in your model.&lt;br&gt;
Hint 1: the `sigmoid` activation function is the inverse of the logit link function. &lt;br&gt;
Hint 2: the `binary_crossentropy` loss maximizes the loglikelihood of Bernoulli outcomes:
$$ \sum_{i=1}^n (\color{#FFA500}{y_i} \cdot \log(p_i) + (1-\color{#FFA500}{y_i}) \cdot \log(1-p_i)).$$

* .hi-pink[Q.2]: .KULbginline[fit] your NN on the outcome or target variable (nclaims &gt; 0), i.e., modeling no claim versus having at least one claim. 

* .hi-pink[Q.3]: .KULbginline[compare] your fitted neural network with a .hi-pink[GLM].
]

---
class:clear

.pull-left[
.hi-pink[Q.1]: neural network architecture for .KULbginline[binary classification]

```r
nn_binary &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'sigmoid', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('accuracy'))
```

.hi-pink[Q.2]: .KULbginline[fit] your neural network

```r
nn_binary %&gt;% fit(x = intercept,
                  y = counts &gt; 0,
                  epochs = 40,
                  batch_size = 1024, 
                  validation_split = 0,
                  verbose = 0)
```


]

.pull-right[
.hi-pink[Q.3]: compare with a .KULbginline[GLM]

```r
glm_binary &lt;- glm((nclaims &gt; 0) ~ 1, 
                  data = mtpl_train, 
                  family = binomial(link = 'logit'))

glm_binary$coefficients
## (Intercept) 
##   -2.073876
nn_binary$get_weights()[[1]] %&gt;% as.numeric()
## [1] -2.063514

unique(predict(glm_binary, type = 'response'))
## [1] 0.111662
unique(predict(nn_binary, x = intercept))
## 3826/3826 - 3s - 3s/epoch - 704us/step
##          [,1]
## [1,] 0.112694
```

]
---
# Taking exposure into account in a neural network

.pull-left[
The .KULbginline[Poisson loss] function, including .hi-pink[exposure], is

$$ \mathcal{L}(y, \hat{y}) = \sum_i \texttt{expo}_i \cdot \lambda_i - y_i \cdot \log(\texttt{expo}_i \cdot \lambda_i),$$
where `\(\hat{y}_i = \text{expo}_i \cdot \lambda_i\)` which is proportional to:

$$ \mathcal{L}(y,\hat{y}) = \sum_i \texttt{expo}_i \cdot (\lambda_i - \frac{y_i}{\texttt{expo}_i} \log(\lambda_i)).$$
This is the loss function for a Poisson model with:

* observations `\(\frac{y_i}{\texttt{expo}_i}\)` and
* weights `\(\texttt{expo}_i\)`.
]

.pull-right[
Notice indeed how the parameter estimates of the following two GLMs are .hi-pink[identical]:

```r
glm_offset &lt;- glm(nclaims ~ ageph,
                  family = poisson(link = 'log'),
                  data = mtpl_train,
                  offset = log(expo))
glm_offset$coefficients
## (Intercept)       ageph 
## -1.22357710 -0.01644835

glm_weights &lt;- glm(nclaims / expo ~ ageph,
                   family = poisson(link = 'log'),
                   data = mtpl_train,
                   weights = expo)
glm_weights$coefficients
## (Intercept)       ageph 
## -1.22357710 -0.01644835
```
]

---
# Taking exposure into account in a neural net (cont.)

.pull-left[
.KULbginline[Nothing] changes in our neural network model .hi-pink[specification]:

```r
nn_freq_exposure &lt;- 
  keras_model_sequential() %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              input_shape = c(1), 
              use_bias = FALSE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```

It is however .hi-pink[good practice] to always .KULbginline[recompile]. &lt;br&gt; &lt;br&gt;
Otherwise the neural network will pick up where it left off last time, with the optimal weights after fitting.

]

.pull-right[
Create a .hi-pink[vector] with exposure values:

```r
exposure &lt;- mtpl_train$expo
```

Divide claim counts by exposure and use weights:

```r
nn_freq_exposure %&gt;% 
  fit(x = intercept,
*     y = counts / exposure,
*     sample_weight = exposure,
      epochs = 20,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```

.KULbginline[Stay tuned] to find out how to include exposure via an .hi-pink[offset] term!

]

---
# Adding an input feature and a hidden layer

.pull-left[
Let's start by adding .hi-pink[one feature], namely `ageph`:

```r
ageph &lt;- mtpl_train$ageph
```

Define the neural network .hi-pink[architecture] with a hidden layer:

```r
nn_freq_ageph &lt;- 
  keras_model_sequential() %&gt;%
  layer_batch_normalization(input_shape = c(1)) %&gt;%
  layer_dense(units = 5,
              activation = 'tanh') %&gt;%
  layer_dense(units = 1, 
              activation = 'exponential', 
              use_bias = TRUE) %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_rmsprop())
```
]

.pull-right[
* .KULbginline[Pre-processing] (see Module 1): 

`layer_batch_normalization` .hi-pink[centers] and .hi-pink[scales] the input features (here only one) .hi-pink[per mini-batch].

* .KULbginline[Hidden] layer:

`layer_dense` with five nodes and the `tanh` activation function.

* .KULbginline[Output] layer:

`layer_dense` with one node and the `exponential` activation function.&lt;br&gt; &lt;br&gt;
Notice how we set `use_bias = TRUE` for the .hi-pink[intercept.]

]

---
# Adding an input feature and a hidden layer (cont.)

.pull-left[
Let's .hi-pink[fit] our brand new neural net:

```r
nn_freq_ageph %&gt;% 
* fit(x = ageph,
      y = counts / exposure,
      sample_weight = exposure,
      epochs = 30,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```



We also fit a .hi-pink[GAM] with a smooth effect for `ageph`:

```r
library(mgcv)
gam_ageph &lt;- gam(nclaims ~ s(ageph),
                 data = mtpl_train, 
                 family = poisson(link = 'log'), 
                 offset = log(expo))
```

.hi-pink[Q.]: What do you think about those fits?
]

.pull-right[

```
## 3/3 - 0s - 115ms/epoch - 38ms/step
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-104-1.png" style="display: block; margin: auto;" /&gt;


]
---

# Adding a skip connection in a neural network

So far, we stayed in a .hi-pink[purely sequential] architecture with `keras_model_sequential()`.

Now, we will allow some input nodes to be connected directly to the output node, i.e., .KULbginline[skip connections].

.pull-left[
&lt;img src="img/CANNarchitecture.png" width="75%" style="display: block; margin: auto;" /&gt;
Figure taken from [Schelldorfer and Wuthrich (2019)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525).
]

.pull-right[
The output node, without skip connection, calculates:
`$$f_{activation}(\sum_i w_i h_i + b).$$`

With a skip connection, this simply becomes:
`$$f_{activation}(\sum_i w_i h_i + b + s).$$`
We take a .hi-pink[linear] combination of the last hidden layer outputs and .hi-pink[add] the skip input, .hi-pink[before] applying the activation function.

So, what can we do with this?
]

---

# Adding a skip connection in a neural network (cont.)

Let's take a .KULbginline[claim frequency] example with the `exponential` activation function.

* Adding exposure as an .hi-pink[offset] term:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(expo)}) = \color{#e64173}{expo} \cdot \exp(\sum_i w_i h_i + b).$$`

* Adding a .hi-pink[base] prediction:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(base)}) = \color{#e64173}{base} \cdot \exp(\sum_i w_i h_i + b).$$`

* The .hi-pink[combination] of both:
`$$output = \exp(\sum_i w_i h_i + b + \color{#e64173}{\log(expo \cdot base)}) = \color{#e64173}{expo \cdot base} \cdot \exp(\sum_i w_i h_i + b).$$`

A skip connection allows us to guide the neural net in the right direction and to model .hi.pink[adjustments] on top of the base predictions, for example obtained via a GLM or GAM. In the actuarial lingo this is called a .KULbginline[C]ombined .KULbginline[A]ctuarial .KULbginline[N]eural .KULbginline[N]etwork (.hi-pink[CANN]).

---

# Adding a skip connection in a neural network (cont.)

Structure of the CANN model we are going to implement

&lt;img src="img/structure_cann.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Adding a skip connection in a neural network (cont.)

Construct the first neural network for the input covariate `age`. 

Apply no activation function in the final layer (= linear activation function). 

The exponential activation function will be applied later after adding the value from the skip connection.


```r
input_nn &lt;- layer_input(shape = c(1), name = 'nn')

output_nn &lt;- input_nn %&gt;% 
  layer_dense(units = 5, activation = 'tanh') %&gt;%
* layer_dense(units = 1, activation = NULL)
```



Create an input for the skip connection.

```r
input_skip &lt;- layer_input(shape = c(1), name = 'skip')
```



---

# Adding a skip connection in a neural network (cont.)

.hi-pink[Combine] the neural network and skip connection via `layer_add` and pass through the `exponential` function with fixed weights (bias zero, weights 1):


```r
output &lt;- list(output_nn, input_skip) %&gt;% 
  layer_add() %&gt;% 
  layer_dense(units = 1, 
              activation = 'exponential',
*             trainable = FALSE,
              name = 'output',
*             weights = list(array(1, dim = c(1,1)), array(0, dim = c(1))))
```




Define the full model with .hi-pink[inputs] and .hi-pink[output] via `keras_model` and .hi-pink[compile] as usual:


```r
cann &lt;- keras_model(inputs = list(input_nn, input_skip), outputs = output)
cann %&gt;% compile(loss = 'poisson', optimize = optimizer_rmsprop())
```

---

# Adding a skip connection in a neural network (cont.)

.pull-left[

&lt;img src="img/CANN_skip_deepviz.png" width="50%" style="display: block; margin: auto;" /&gt;



]

.pull-right[
Calculate the GAM .hi-pink[base] predictions, including .hi-pink[exposure]:

```r
gam_expo &lt;- predict(gam_ageph) + log(mtpl_train$expo)
```

Collect the CANN input data in a .hi-pink[named list]:

```r
cann_input &lt;- list('nn' = mtpl_train$ageph,
                   'skip' = gam_expo)
```

.hi-pink[Fit] the CANN like we have seen before:

```r
cann %&gt;% fit(x = cann_input,
             y = counts,
             epochs = 20,
             batch_size = 1024, 
             validation_split = 0,
             verbose = 0)
```
]



---

# Adding a skip connection in a neural network (cont.)

.pull-left[

```
## 3/3 - 0s - 51ms/epoch - 17ms/step
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-119-1.png" style="display: block; margin: auto;" /&gt;

]

.pull-right[

```
## 3/3 - 0s - 44ms/epoch - 15ms/step
```

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-120-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Claim severity modeling with neural nets

.pull-left[

Filter the .hi-pink[claims] data:

```r
claims &lt;- mtpl_train %&gt;% dplyr::filter(nclaims &gt; 0)
```


Let's model the .hi-pink[log severity] with a .hi-pink[MSE] loss:

```r
nn_sev_log &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 1, activation = 'linear', 
              input_shape = c(1), use_bias = FALSE) %&gt;%
* compile(loss = 'mse',
          optimize = optimizer_rmsprop())

nn_sev_log %&gt;% 
  fit(x = rep(1, nrow(claims)),
*     y = log(claims$avg),
      epochs = 100, batch_size = 128, 
      validation_split = 0, verbose = 0)

predict(nn_sev_log, 1) %&gt;% exp() %&gt;% as.numeric()
## 1/1 - 0s - 29ms/epoch - 29ms/step
## [1] 456.2719
```
]

.pull-right[
Calculate the .hi-pink[empirical] mean and median severity:

```r
claims$avg %&gt;% mean()
## [1] 1689.269
claims$avg %&gt;% median()
## [1] 542.924
```


Seems like we are heavily .KULbginline[underestimating] the mean?

The mean and median of the .hi-pink[lognormal] distribution:
`$$\text{mean} = \exp(\mu + \sigma^2/2)$$`
`$$\text{median} = \exp(\mu)$$`
We are doing the latter, but how to .hi-pink[estimate] `\(\sigma\)` in a neural network?

&lt;br&gt;


]


---
# Claim severity modeling with neural nets (cont.)

.pull-left[
.KULbginline[Alternatives] to consider:

* use MSE with .KULbginline[untransformed] severity amounts?

* implement .KULbginline[your own loss] function (e.g., gamma).


The .hi-pink[gamma deviance] for observation `\(i\)` is defined as:

`$$\frac{y_i - \hat{y}_i}{\hat{y}_i} - \ln \left( \frac{y_i}{\hat{y}_i} \right).$$`
Keras takes care of averaging over observations internally.

Note that we use an `exponential` activation to stay in line with a gamma GLM with .hi-pink[log-link] function.


```r
claims %&gt;% dplyr::filter(avg &lt; 300000) %&gt;% 
  dplyr::pull(avg) %&gt;% mean()
## [1] 1460.887
```

]

.pull-right[

```r
k_gamma &lt;- function(y_true, y_pred) { 
((y_true - y_pred) / y_pred) - k_log(y_true / y_pred)
}
```


```r
nn_sev_gamma &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 1,
*       activation = 'exponential',
        input_shape = c(1), use_bias = FALSE) %&gt;%
* compile(loss = k_gamma,
          optimize = optimizer_rmsprop())

nn_sev_gamma %&gt;% 
  fit(x = rep(1, nrow(claims)),
*     y = claims$avg,
      epochs = 100, batch_size = 128, 
      validation_split = 0, verbose = 0)

predict(nn_sev_gamma, 1) %&gt;% as.numeric()
## 1/1 - 0s - 29ms/epoch - 29ms/step
## [1] 1471.757
```

]

---

# Background reading

.left-column[

&lt;br&gt;

&lt;img src = "img/naaj.png" height = "350px" /&gt;

]

.right-column[

In module 2 we mentioned the Henckaerts et al. (2021) paper on [Boosting insights in insurance tariff plans with tree-based machine learning methods](https://katrienantonio.github.io/project/tree-based-insurance-pricing/).

We extended this paper to deep learning structures in Holvoet et al. (2023, arxiv) on [Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff](https://arxiv.org/abs/2310.12671): 

- full algorithmic details and tuning strategy for ANNs, CANNs (fixed weights and trainable weights), with focus on claim frequency and severity modelling
- comparison of embedding strategies for categorical (or: factor) covariates: auto-encoders and one-hot encoding
- model comparison (GLMs, GAMs, trees, RFs, GBMs, ANNs, CANNs) 
- interpretation and managerial tools (e.g., loss ratio, discrimination power)
- construction of GLM as a globale surrogate model for a (more complex) deep learning model. 

The paper comes with reproducible examples in [https://github.com/freekholvoet/nnforfreqsevpricing](https://github.com/freekholvoet/nnforfreqsevpricing).


]

---

# Contributions to the literature

.center[
&lt;img src="./img/literature_overview.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;
]

---

# Data sets in the Holvoet et al. (2023) study

.center[
&lt;img src="./img/data_sets_NN.png" width="80%" height="60%" style="display: block; margin: auto;" /&gt;
]



---

# Background reading: Holvoet et al. (2023, arxiv)

.left-column[


&lt;br&gt;

&lt;img src = "img/randomgrid_example.png" width = "800px" /&gt;

]

.right-column[

We use random grid search as tuning strategy (Bergstra and Bengio, 2012):

* for each tuning parameter `\(t_k\)` we define a range `\([t_{k,\text{min}}, t_{k,\text{max}}]\)`

* the search space `\(\mathcal{S}\)` is defined as 
  `\(\mathcal{S} = \left[t_{1,\text{min}},t_{1,\text{max}}\right] \times\ldots\times\left[t_{K,\text{min}},t_{K,\text{max}}\right]\)`
  
* we draw a random grid `\(\mathcal{R}\subset\mathcal{S}\)` of candidate tuning parameters.

]

---

# Background reading: Holvoet et al. (2023, arxiv)

For both the ANNs and CANNs, a random grid `\(\mathcal{R}\)` of size 40 is sampled from the
search space `\(\mathcal{S}\)` defined by the tuning parameters and their respective ranges as shown below:

&lt;img src="img/tuning_holvoet_paper.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Out-of-sample evaluation in Holvoet et al. (2023, arxiv)


.center[
&lt;img src="./img/pred_comp_NN.png" width="60%" height="60%" style="display: block; margin: auto;" /&gt;
]

Out-of-sample performance comparison between the different models for the Australian and Belgian data sets. The LHS shows the performance of the frequency models and the RHS for
the severity models. 

.center[
&lt;img src="./img/legend_CANN.png" width="60%" height="60%" style="display: block; margin: auto;" /&gt;
]

---

# Out-of-sample evaluation (cont'd)

.center[
&lt;img src="./img/pred_comp_NN_2.png" width="60%" height="60%" style="display: block; margin: auto;" /&gt;
]


Out-of-sample performance comparison between the different models for the French and Norwegian data sets. The LHS shows the performance of the frequency models and the RHS for
the severity models. 

---


class: inverse, center, middle
name: cnn

# Convolutional neural networks (CNNs)

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;

---
# The problems with flattening
  
.pull-left[
With ANNs, our first step in the MNIST analysis was to .KULbginline[flatten] the image matrix into a vector:
    

```r
input &lt;- tensorflow::array_reshape(
  input, c(nrow(input), 28*28)) / 255
```
    
    
This approach
  * is not .hi-pink[translation] invariant. A completely different set of nodes gets activated when the image is shifted.
    
  * ignores the .hi-pink[dependency] between nearby pixels.
    
  * requires a .hi-pink[large number] of parameters/weights as each node in the first hidden layer is connected to all nodes in the input layer.

]

.pull-right[
&lt;img src="img/neural_network_flattening.png" width="300" height="350" style="display: block; margin: auto;" /&gt;
  
.right[
Source: [Sumit Saha](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)]

.KULbginline[Convolutional layers] allow to handle multi-dimensional data, .hi-pink[without] flattening.
]



---
# Convolutional layers

Classical hidden layers (as we have seen so far) use .hi-pink[1 dimensional inputs] to construct .hi-pink[1 dimensional features].

.KULbginline[2d convolutional] layers use .hi-pink[2 dimensional input] (for example images) to construct .hi-pink[2 dimensional feature maps].

.pull-left[
The weights in a 2d convolutional layer are structured in a small image, called the .hi-pink[kernel] or the .hi-pink[filter].

&lt;img src="img/neural_network_convolution_op1.png" width="300" height="200" style="display: block; margin: auto;" /&gt;

]

.pull-right[
We slide the kernel over the input image, multiply the selected part of the image and the kernel elementwise and sum:

&lt;img src="img/neural_network_convolution_op2.png" width="400" height="250" style="display: block; margin: auto;" /&gt;

.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]
]

---
# Convolutional layers (cont.)

Classical hidden layers (as we have seen so far) use .hi-pink[1 dimensional inputs] to construct .hi-pink[1 dimensional features].

.KULbginline[2d convolutional] layers use .hi-pink[2 dimensional input] (for example images) to construct .hi-pink[2 dimensional feature maps].

.pull-left[
The weights in a 2d convolutional layer are structured in a small image, called the .hi-pink[kernel] or the .hi-pink[filter].

&lt;img src="img/neural_network_convolution_op1.png" width="300" height="200" style="display: block; margin: auto;" /&gt;

]

.pull-right[
We slide the kernel over the input image, multiply the selected part of the image and the kernel elementwise and sum:

&lt;img src="img/neural_network_convolution_op3.gif" width="400" height="250" style="display: block; margin: auto;" /&gt;
.right[Source: [Bradley Boehmke](https://github.com/rstudio-conf-2020/dl-keras-tf)]
]

---

# Convolutional layers (cont.)

.KULbginline[2d convolutional] layers can .hi-pink[detect] the same, local feature .hi-pink[anywhere] in the image.

.pull-left[
A useful feature for classifying the number four is the presence of straight, .hi-pink[vertical lines].

.hi-pink[Q]: How should the .hi-pink[kernel] look to detect this feature?
  
]

.pull-right[
original image:
&lt;img src="img/neural_network_mnist4.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
]

---
# Convolutional layers (cont.)

.KULbginline[2d convolutional] layers can .hi-pink[detect] the same, local feature .hi-pink[anywhere] in the image.

.pull-left[
A useful feature for classifying the number four is the presence of straight, .hi-pink[vertical lines].

.hi-pink[Q]: How should the .hi-pink[kernel] look to detect this feature?
  
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-140-1.png" width="200" height="200" style="display: block; margin: auto;" /&gt;


]

.pull-right[
original image:
&lt;img src="img/neural_network_mnist4.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
feature map:
&lt;img src="img/neural_network_feature_map.png" width="200" height="200" style="display: block; margin: auto;" /&gt;
]

---
# Convolutional layers in {keras}

.pull-left[

Add a .KULbginline[2d convolutional layer] with `layer_conv_2d()`:

 

```r
keras_model_sequential() %&gt;%
* layer_conv_2d(filters = 8,
*               kernel_size = c(3, 3),
*               strides = c(1, 1),
*               input_shape = c(28, 28, 1))
```
  
 
]

.pull-right[
* `filters = 8`:
  
  We construct .hi-pink[8 feature maps] associated to different kernels/.hi-pink[filters].

* `kernel_size = c(3, 3)`:
  
  The filter/.hi-pink[kernel] has a size of .hi-pink[3x3].

* `strides = c(1, 1)`:
  
  We .hi-pink[move] the .hi-pink[kernel] in steps of .hi-pink[1] pixel in both the horizontal and vertical direction. This is a common choice.

* `input_shape = c(28, 28, 1)`:
  
  If this is the first layer of the model, we also have to specify the dimensions of the input data. The input consists of .hi-pink[1 image] of size .hi-pink[28x28].
]


---
# Pooling layers

A convolution layer is typically followed by a .hi-pink[pooling step], which reduces the size of feature maps.

.pull-left[
.KULbginline[Pooling layers] divide the image in blocks of equal size and then .hi-pink[aggregate] the data per block.

Two common operations are:
  
  * average pooling

```r
layer_average_pooling_2d(pool_size = c(2, 2),
                         strides = c(2, 2))
```

* max pooling

```r
layer_max_pooling_2d(pool_size = c(2, 2),
                     strides = c(2, 2))
```


]

.pull-right[
&lt;img src="img/neural_network_pooling.jpg" width="380" height="240" style="display: block; margin: auto;" /&gt;

* `pool_size = c(2, 2)`: 
  
  Pool blocks of 2x2

* `strides = c(2, 2)`:
  
  Move in steps of size 2 in both the horizontal and vertical direction.
  
]

---
# Flattening layers

.pull-left[
When all features are extracted, the data is .KULbginline[flattened]. 

This data can be seen as .hi-pink[engineered features], automatically created by the CNN architecture.

&lt;br&gt;

In a next step, a .hi-pink[feed-forward ANN] is used to analyze these local features.


]

.pull-right[

```r
keras_model_sequential() %&gt;%
  layer_conv_2d() %&gt;%
  layer_max_pooling_2d() %&gt;%
* layer_flatten()
```


```r
keras_model_sequential() %&gt;%
  layer_conv_2d() %&gt;%
  layer_max_pooling_2d() %&gt;%
  layer_flatten() %&gt;%
* layer_dense() %&gt;%
* layer_dense() %&gt;%
  compile()
```
]

---
# A CNN architecture

&lt;br&gt; 

&lt;img src="https://iq.opengenus.org/content/images/2019/04/pic01-1.png" style="display: block; margin: auto;" /&gt;

---
# A CNN with the MNIST data

.pull-left[
The image .hi-pink[input] data is not flattened this time:

```r
load("../data/mnist.RData")
input_train &lt;- mnist$train$x / 255
input_test &lt;- mnist$test$x / 255
```

We need to expand the axis with one extra dimension:

```r
dim(input_train)
## [1] 60000    28    28
input_train &lt;- k_expand_dims(input_train)
dim(input_train)
## [1] 60000    28    28     1
input_test &lt;- k_expand_dims(input_test)
```

The .hi-pink[output] labels are one-hot encoded like before:

```r
output_train &lt;- keras::to_categorical(mnist$train$y, 10)
output_test &lt;- keras::to_categorical(mnist$test$y, 10)
```


]

.pull-right[
Let's fit a CNN to the MNIST data:

```r
model &lt;- keras_model_sequential() %&gt;%
    layer_conv_2d(filters = 8, 
                  kernel_size = 3, 
                  input_shape = c(28, 28, 1)) %&gt;%
    layer_max_pooling_2d(pool_size = 2,
                         strides = 2) %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 10, 
                activation = 'softmax') %&gt;%
    compile(loss = 'categorical_crossentropy',
            optimize = optimizer_rmsprop(),
            metrics = c('accuracy')) 
  
model %&gt;% fit(x = input_train,
              y = output_train, 
              epochs = 10, 
              batch_size = 128, 
              validation_split = 0.2,
              verbose = 0)
```



]

---
# Model evaluation

.pull-left[
Let's .KULbginline[evaluate] the model on our .hi-pink[test] data:

```r
model %&gt;% evaluate(input_test,
                   output_test,
                   verbose = 0)
##     loss accuracy 
## 0.105472 0.968700
```
Almost 97% of the test images are now correctly classified!

Which images are misclassified?

```r
prediction &lt;- model %&gt;% predict(input_test)
## 313/313 - 1s - 719ms/epoch - 2ms/step
category &lt;- apply(prediction, 1, which.max) - 1
actual_category &lt;- apply(output_test, 1, which.max) - 1
head(which(actual_category != category))
## [1]   9  93 248 260 291 306
```
]

.pull-right[

```r
cbind(category, actual_category)[93,]
##        category actual_category 
##               4               9
```
&lt;img src="ML_part3_files/figure-html/unnamed-chunk-158-1.png" width="75%" style="display: block; margin: auto;" /&gt;
]

---
# Inspecting the filter/kernel

The 8 3x3 .KULbginline[filters] can be extracted from the network via `$get_weights()`:

```r
filters &lt;- model$get_weights()[[1]]
str(filters)
##  num [1:3, 1:3, 1, 1:8] -0.572 0.248 0.363 -0.461 0.518 ...
```

.hi-pink[Q:] can you find an interpretation for each of these filters?

&lt;img src="ML_part3_files/figure-html/unnamed-chunk-160-1.png" width="30%" style="display: block; margin: auto;" /&gt;

---
name: yourturn
class: clear

.left-column[
  
&lt;!-- Add icon library --&gt;
  &lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;
  
  ## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn
    
]


.right-column[
Time for you to .KULbginline[experiment] with .hi-pink[CNNs] in {keras}. Why not try to achieve 98% accuracy?

&lt;br&gt;
  
We have now built .KULbginline[convolutional] neural networks using .hi-pink[layer_conv_2d()]. &lt;br&gt;
In addition, {keras} offers .hi-pink[layer_conv_1d()] and .hi-pink[layer_conv_3d()]. &lt;br&gt;

.hi-pink[Q]: For which data would you use .hi-pink[layer_conv_1d()]? &lt;br&gt;

.hi-pink[Q]: For which data would you use .hi-pink[layer_conv_3d()]?

]
---
class: inverse, center, middle
name: autoencoder

# Auto encoders

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#FAFAFA' size=1px width=796px&gt;&lt;/html&gt;
  
---
# Auto encoders
  
.KULbginline[Auto encoders] compress the input data into a .hi-pink[limited number of features]. 

.pull-left[
&lt;img src="img/neural_network_auto_encoder.png" width="300" height="400" style="display: block; margin: auto;" /&gt;
]

.pull-right[

* .KULbginline[Unsupervised] machine learning algorithm. 

* .KULbginline[Dimension reduction] of the input data, comparable with PCA. The low dimensional compressed data is often used as an input in predictive models.

* Input and output are identical.

* Few nodes in the center of the network. This is the compressed feature space.

* A high performing auto encoder is capable of reconstructing the input data based on compressed feature space.
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
  &lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;
  
  ## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn
    
]


.right-column[
  
Auto encoders can be implemented in Keras using the same tools that you have already learned during this course. 
&lt;br&gt;
The following steps guide you in .KULbginline[constructing] and .KULbginline[training] your personal auto encoder for the MNIST dataset.

* Make a sketch of the neural network that you will implement.

* Define a neural network with 5 layers:
  * Layer 1: input (784 nodes)
  * Layer 2: Hidden layer (128 nodes)
  * Layer 3: Hidden layer (32 nodes), this is the compressed feature space
  * Layer 4: Hidden layer (128 nodes)
  * Layer 5: Output layer (784 nodes)

* Choose an appropriate activation function for each layer:
  * identity
  * ReLU
  * sigmoid
  * softmax
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
  &lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;
  
  ## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn
    
]


.right-column[

* Which of these loss functions can we use to train the model?
  * mse
  * binary_crossentropy
  * categorical_crossentropy

* Fit the model on the MNIST data in 10 epochs.

* Experiment with adding other layer types to the model:
  * layer_gaussian_noise(stddev)
  * layer_dropout(rate)
  * layer_batch_normalization()
]

---
class: clear

.pull-left[

```r
*encoder &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = 'sigmoid')

*model &lt;- encoder %&gt;%
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid') %&gt;%
  layer_dense(units = 784, activation = 'sigmoid') %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
  fit(input, 
      input, 
      epochs = 10, 
      batch_size = 256, 
      shuffle = TRUE, 
      validation_split = 0.2)
```
]

.pull-right[
`encoder` contains the first part of the model for compressing the model.
  
`model` is the full auto encoder, including the encode and decode step.
  
By defining `model` as an extension of `encoder`, we can compress the data using `predict(encoder, ...)` after training the model.
]

---
class: clear

.pull-left[

```r
encoder &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 128, activation = `'sigmoid'`, 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = `'sigmoid'`)

model &lt;- encoder %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = `'sigmoid'`) %&gt;%
  layer_dense(units = 784, activation = `'sigmoid'`) %&gt;%
  compile(loss = `'binary_crossentropy'`,
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
  fit(input, 
      input, 
      epochs = 10, 
      batch_size = 256, 
      shuffle = TRUE, 
      validation_split = 0.2)
```
]

.pull-right[
I interpret the hidden nodes as binary features and therefore use a `sigmoid` activation function. 

We no longer use the `softmax` activation function in the last layer, since multiple output nodes can be activated simultaneously.

I choose `binary_crossentropy` as a loss function, since we have independent bernoulli outcome variables.

Another good combination would have been:
* activation `ReLU` in the hidden layers
* activation `identity` in the output layer
* `mse` as the loss function
]

---
class:clear

.pull-left[

```r
encoder &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 128, activation = 'sigmoid', 
              input_shape = c(784)) %&gt;%
  layer_dense(units = 32, activation = 'sigmoid')

model &lt;- encoder %&gt;% 
  layer_batch_normalization() %&gt;%
  layer_dense(units = 128, activation = 'sigmoid') %&gt;%
  layer_dense(units = 784, activation = 'sigmoid') %&gt;%
  compile(loss = 'binary_crossentropy',
          optimize = optimizer_rmsprop(),
          metrics = c('mse'))

model %&gt;%
* fit(input,
*     input,
      epochs = 10, 
      batch_size = 256, 
*     shuffle = TRUE,
      validation_split = 0.2)
```
]

.pull-right[
The `input` variable is also passed to the model as the `output` parameter.
  
The option `shuffle=TRUE` shuffles the training dataset after each epoch, such that the model is trained on different batches.
  
]

---
# The big test
  
Let's compare the input and output of our auto encoder.


```r
result &lt;- predict(model, input[1, , drop = FALSE])
plot_image(input[1, ]) # the original image
plot_image(result[1, ]) # the reconstruction of the model
```


&lt;img src="img/neural_network_auto_encoder_comparison.png" width="750" height="370" style="display: block; margin: auto;" /&gt;


---
# What happens with random noise?


```r
random &lt;- matrix(runif(28^2), nrow = 1)

gridExtra::grid.arrange(
  plot_image(random[1, ]) + theme(legend.position = 'none'),
  plot_image(predict(model, random)[1, ]) + theme(legend.position = 'none'),
  nrow = 1)
```

&lt;img src="img/neural_network_auto_encoder_noise.png" width="500" height="300" style="display: block; margin: auto;" /&gt;

---

name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

Time to .KULbginline[apply] everything you learned on neural networks in a .KULbginline[claim frequency case study]! &lt;br&gt;
The goal is to .hi-pink[fit] a couple of neural nets on `mtpl_train` and to .hi-pink[evaluate] those models on `mtpl_test`.

**Step 1**: master .KULbginline[data pre-processing]!

* Write a .hi-pink[recipe] to prepare the input data.  &lt;br&gt; Feel free to check the {recipes} [reference page](https://tidymodels.github.io/recipes/reference/index.html) for inspiration.

* Your recipe should contain at least the following .hi-pink[steps]:&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; remove the features `id`, `amount`, `avg`, `town` and `pc` with `step_rm`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; normalize all numeric features with `step_normalize`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; dummy encode all nominal features with `step_dummy`.&lt;br&gt;

* .hi-pink[Bake] the training and test data following your recipe.

* Make the train and test data .hi-pink[NN proof] as follows:&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; create .hi-pink[vectors] for both `nclaims` and `expo`.&lt;br&gt;
&amp;nbsp;&amp;nbsp;&amp;#9675; create a .hi-pink[matrix] for the input features via `as.matrix()`.
]

---
name: yourturn
class: clear

.left-column[

&lt;!-- Add icon library --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

## &lt;i class="fa fa-edit"&gt;&lt;/i&gt; &lt;br&gt; Your turn

]


.right-column[

**Step 2**: let's move to .KULbginline[model building].

* .hi-pink[Compile] and .hi-pink[fit] a couple of neural networks on the pre-processed training data. &lt;br&gt; You are completely free to specify the architecture of your choice. 

* You can .hi-pink[experiment] with the following options:&lt;br&gt;
&amp;#9675; the number of layers,&lt;br&gt;
&amp;#9675; the number of nodes per layer,&lt;br&gt;
&amp;#9675; the batch size,&lt;br&gt;
&amp;#9675; the activation functions,&lt;br&gt;
&amp;#9675; adding dropout layers,&lt;br&gt;
&amp;#9675; including batch normalization after some layers,&lt;br&gt;
&amp;#9675; ...&lt;br&gt;

* If you want to include exposure, either use .hi-pink[weights] or a .hi-pink[skip] connection.

**Step 3**: now continue with the .KULbginline[model evaluation].

* .hi-pink[Evaluate] your models on the test data via `evaluate`. Which architecture is the winner?

* .hi-pink[Compare] your favorite neural network with a GLM, GAM or GBM. Which model is preferred?

]

---

class:clear

.pull-left[
.hi-pink[Step 1]: data pre-processing with {recipes}

```r
library(recipes)

# Create and prepare the recipe
mtpl_recipe &lt;- recipe(nclaims ~ ., data = mtpl_train) %&gt;%
  step_rm(id, amount, avg, town, pc) %&gt;%
  step_nzv(all_predictors(), -expo) %&gt;%
  step_normalize(all_numeric(), -c(nclaims, expo)) %&gt;%
  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%
  prep(mtpl_train)

# Bake the training and test data
mtpl_train_b &lt;- mtpl_recipe %&gt;% juice()
mtpl_test_b &lt;- mtpl_recipe %&gt;% bake(new_data = mtpl_test)

# Make the data NN proof
train_x &lt;- mtpl_train_b %&gt;% 
  dplyr::select(-c(nclaims, expo)) %&gt;% as.matrix()
test_x &lt;- mtpl_test_b %&gt;% 
  dplyr::select(-c(nclaims, expo)) %&gt;% as.matrix()

train_y &lt;- mtpl_train_b$nclaims
test_y &lt;- mtpl_test_b$nclaims

train_expo &lt;- mtpl_train_b$expo
test_expo &lt;- mtpl_test_b$expo
```
]

.pull-right[
.hi-pink[Step 2]: a possible NN architecture with 2 hidden layers

```r
nn_case &lt;- keras_model_sequential() %&gt;% 
  layer_dense(units = 20,
              activation = 'relu', 
              input_shape = ncol(train_x)) %&gt;%
  layer_dense(units = 10,
              activation = 'relu') %&gt;% 
  layer_dense(units = 1, 
              activation = 'exponential') %&gt;%
  compile(loss = 'poisson',
          optimize = optimizer_nadam())
  
nn_case %&gt;%
  fit(x = train_x,
      y = train_y / train_expo,
      sample_weight = train_expo,
      epochs = 20,
      batch_size = 1024, 
      validation_split = 0,
      verbose = 0)
```


]

---

class:clear

.pull-left[
.hi-pink[Step 3a]: evaluate the NN on the test data

```r
# Built-in evaluation
nn_case %&gt;%
  evaluate(x = test_x,
           y = test_y,
           verbose = 0)
##      loss 
## 0.3786413

# If you want to check the results
poisson_loss &lt;- function(pred, actual) {
  mean(pred - actual * log(pred))
}
poisson_loss(predict(nn_case, test_x),
             test_y)
## 1276/1276 - 1s - 1s/epoch - 820us/step
## [1] 0.3786418

# Use array for weights in evaluate
nn_case %&gt;% 
  evaluate(x = test_x,
           y = test_y,
           sample_weight = array(test_expo),
           verbose = 0)
##      loss 
## 0.3456186
```

]

.pull-right[
.hi-pink[Step 3b]: compare the NN performance with a GAM

```r
gam_case &lt;- gam(
  nclaims ~ coverage + fuel + sex +
    s(ageph) + s(bm) + s(power) + s(agec),
  data = mtpl_train,
  offset = log(expo),
  family = poisson(link = 'log')
  )

poisson_loss(predict(gam_case, mtpl_test,
                     type = 'response'),
             test_y)
## [1] 0.3794901
```


]

---

# Background reading

In the Holvoet et al. (2023, arxiv) paper we explore the use of autoencoders as an embedding for categorical (or: factor) variables. 

We also compare the performance of autoencoder embedding and one-hot encoding. 

.center[
&lt;img src="./img/Autoencoder.png" width="80%" height="40%" style="display: block; margin: auto;" /&gt;
]

---
name: wrap-up

# Thanks!  &lt;img src="img/xaringan.png" class="title-hex"&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
&lt;br&gt; &lt;br&gt; &lt;br&gt;
Course material available via 
&lt;br&gt;
<svg aria-hidden="true" role="img" viewBox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#116E8A;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> https://github.com/katrienantonio/hands-on-machine-learning-R-module-3

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"highlightLines": true,
"countIncrementalSlides": false,
"highlightSpans": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
